
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>megengine.module.conv &#8212; MegEngine 1.3.0 文档</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../_static/css/index.93dda2a1e4f2b831d8345b5b3dbee4ea.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.3c6125c0ae68274ddd1b.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/translations.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script src="../../../_static/js/custom.js"></script>
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">


    
      
      <a class="navbar-brand" href="../../../index.html">
        <img src="../../../_static/logo.png" class="logo" alt="logo">
      </a>
      
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../getting-started/index.html">
  新手入门
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../user-guide/index.html">
  用户指南
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../reference/index.html">
  API 参考
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../development/index.html">
  开发者指南
 </a>
</li>

        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://discuss.megengine.org.cn/">论坛<i class="fas fa-external-link-alt"></i></a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://megengine.org.cn/">官网<i class="fas fa-external-link-alt"></i></a>
        </li>
        
      </ul>

      <form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="输入搜索文本..." aria-label="输入搜索文本..." autocomplete="off" >
</form>
      <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/MegEngine/MegEngine" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>

      <script type="text/javascript">
  (function () {
    window.versionSwitcher = {
      pageName: "_modules/megengine/module/conv.html",
      versionJsonUrl: "/doc/version.json",
      enableLocaleSupport: "True" === "True",
      // TODO read from "zh, en"
      allLocales: [
        {
          "locale": "zh",
          "display": "中文"
        },
        {
          "locale": "en",
          "display": "EN"
        }
      ]
    }
  })();
</script>

<ul class="navbar-nav">
  <li class="nav-item dropdown">
    <button id="version-dropdown" class="btn btn-secondary btn-sm dropdown-toggle" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      <!-- placeholder for javascript filling above -->
    </button>
    <div id="version-menu" class="dropdown-menu" style="min-width: 6rem;">
      <!-- placeholder for javascript filling above -->
    </div>
  </li>
  <li class="nav-item">
    <span id="locale-switcher">
      <!-- placeholder for locale switcher -->
    </span>
  </li>
</ul>
      
        </li>
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    
</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>megengine.module.conv 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># MegEngine is Licensed under the Apache License, Version 2.0 (the &quot;License&quot;)</span>
<span class="c1">#</span>
<span class="c1"># Copyright (c) 2014-2021 Megvii Inc. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing,</span>
<span class="c1"># software distributed under the License is distributed on an</span>
<span class="c1"># &quot;AS IS&quot; BASIS, WITHOUT ARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">..functional</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">conv1d</span><span class="p">,</span>
    <span class="n">conv2d</span><span class="p">,</span>
    <span class="n">conv3d</span><span class="p">,</span>
    <span class="n">conv_transpose2d</span><span class="p">,</span>
    <span class="n">deformable_conv2d</span><span class="p">,</span>
    <span class="n">local_conv2d</span><span class="p">,</span>
    <span class="n">relu</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">..tensor</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">..utils.tuple_function</span> <span class="kn">import</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_pair_nonzero</span><span class="p">,</span> <span class="n">_triple</span><span class="p">,</span> <span class="n">_triple_nonzero</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">.module</span> <span class="kn">import</span> <span class="n">Module</span>


<span class="k">class</span> <span class="nc">_ConvNd</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;base class for convolution modules, including transposed conv&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;in_channels must be divisible by groups&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;out_channels must be divisible by groups&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_infer_weight_shape</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_infer_bias_shape</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_get_fanin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fanin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_fanin</span><span class="p">()</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">fanin</span><span class="p">)</span>
        <span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_infer_weight_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_infer_bias_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">_module_info_string</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{in_channels}</span><span class="s2">, </span><span class="si">{out_channels}</span><span class="s2">, kernel_size=</span><span class="si">{kernel_size}</span><span class="s2">&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;, stride=</span><span class="si">{stride}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;, padding=</span><span class="si">{padding}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;, dilation=</span><span class="si">{dilation}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;, groups=</span><span class="si">{groups}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s2">&quot;, bias=False&quot;</span>
        <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conv1d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>

    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 1D convolution over an input tensor.</span>

<span class="sd">    For instance, given an input of the size :math:`(N, C_{\text{in}}, H)`,</span>
<span class="sd">    this layer generates an output of the size</span>
<span class="sd">    :math:`(N, C_{\text{out}}, H_{\text{out}})` through the</span>
<span class="sd">    process described as below:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</span>

<span class="sd">    where :math:`\star` is the valid 1D cross-correlation operator,</span>
<span class="sd">    :math:`N` is batch size, :math:`C` denotes number of channels, and</span>
<span class="sd">    :math:`H` is length of 1D data element.</span>


<span class="sd">    When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="sd">    where K is a positive integer, this operation is also known as depthwise</span>
<span class="sd">    convolution.</span>

<span class="sd">    In other words, for an input of size :math:`(N, C_{in}, H_{in})`,</span>
<span class="sd">    a depthwise convolution with a depthwise multiplier `K`, can be constructed</span>
<span class="sd">    by arguments :math:`(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})`.</span>

<span class="sd">    :param in_channels: number of input channels.</span>
<span class="sd">    :param out_channels: number of output channels.</span>
<span class="sd">    :param kernel_size: size of weight on spatial dimensions. If kernel_size is</span>
<span class="sd">        an :class:`int`, the actual kernel size would be</span>
<span class="sd">        `(kernel_size, kernel_size)`. Default: 1</span>
<span class="sd">    :param stride: stride of the 1D convolution operation. Default: 1</span>
<span class="sd">    :param padding: size of the paddings added to the input on both sides of its</span>
<span class="sd">        spatial dimensions. Only zero-padding is supported. Default: 0</span>
<span class="sd">    :param dilation: dilation of the 1D convolution operation. Default: 1</span>
<span class="sd">    :param groups: number of groups into which the input and output channels are divided,</span>
<span class="sd">        so as to perform a &quot;grouped convolution&quot;. When ``groups`` is not 1,</span>
<span class="sd">        ``in_channels`` and ``out_channels`` must be divisible by ``groups``,</span>
<span class="sd">        and there would be an extra dimension at the beginning of the weight&#39;s</span>
<span class="sd">        shape. Specifically, the shape of weight would be `(groups,</span>
<span class="sd">        out_channel // groups, in_channels // groups, *kernel_size)`.</span>
<span class="sd">    :param bias: whether to add a bias onto the result of convolution. Default:</span>
<span class="sd">        True</span>
<span class="sd">    :param conv_mode: Supports `CROSS_CORRELATION`. Default:</span>
<span class="sd">        `CROSS_CORRELATION`</span>
<span class="sd">    :param compute_mode: When set to &quot;DEFAULT&quot;, no special requirements will be</span>
<span class="sd">        placed on the precision of intermediate results. When set to &quot;FLOAT32&quot;,</span>
<span class="sd">        &quot;Float32&quot; would be used for accumulator and intermediate result, but only</span>
<span class="sd">        effective when input and output are of float16 dtype.</span>

<span class="sd">    Examples:</span>

<span class="sd">    .. testcode::</span>

<span class="sd">        import numpy as np</span>
<span class="sd">        import megengine as mge</span>
<span class="sd">        import megengine.module as M</span>

<span class="sd">        m = M.Conv1d(in_channels=3, out_channels=1, kernel_size=3)</span>
<span class="sd">        inp = mge.tensor(np.arange(0, 24).astype(&quot;float32&quot;).reshape(2, 3, 4))</span>
<span class="sd">        oup = m(inp)</span>
<span class="sd">        print(oup.numpy().shape)</span>

<span class="sd">    Outputs:</span>

<span class="sd">    .. testoutput::</span>

<span class="sd">        (2, 1, 2)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">conv_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CROSS_CORRELATION&quot;</span><span class="p">,</span>
        <span class="n">compute_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;DEFAULT&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span> <span class="o">=</span> <span class="n">conv_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_mode</span> <span class="o">=</span> <span class="n">compute_mode</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_fanin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">kh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="n">ic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="k">return</span> <span class="n">kh</span> <span class="o">*</span> <span class="n">ic</span>

    <span class="k">def</span> <span class="nf">_infer_weight_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
        <span class="n">ichl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="n">ochl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span>
        <span class="n">kh</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="k">if</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Assume format is NCH(W=1)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">ochl</span><span class="p">,</span> <span class="n">ichl</span><span class="p">,</span> <span class="n">kh</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">ichl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ochl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;invalid config: input_channels=</span><span class="si">{}</span><span class="s2"> output_channels=</span><span class="si">{}</span><span class="s2"> group=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">ichl</span><span class="p">,</span> <span class="n">ochl</span><span class="p">,</span> <span class="n">group</span>
        <span class="p">)</span>
        <span class="c1"># Assume format is NCH(W=1)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">ochl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">ichl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">kh</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_bias_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Assume format is NCH(W=1)</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">calc_conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">conv1d</span><span class="p">(</span>
            <span class="n">inp</span><span class="p">,</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_mode</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">calc_conv</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D convolution over an input tensor.</span>

<span class="sd">    For instance, given an input of the size :math:`(N, C_{\text{in}}, H, W)`,</span>
<span class="sd">    this layer generates an output of the size</span>
<span class="sd">    :math:`(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})` through the</span>
<span class="sd">    process described as below:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</span>

<span class="sd">    where :math:`\star` is the valid 2D cross-correlation operator,</span>
<span class="sd">    :math:`N` is batch size, :math:`C` denotes number of channels,</span>
<span class="sd">    :math:`H` is height of input planes in pixels, and :math:`W` is</span>
<span class="sd">    width in pixels.</span>

<span class="sd">    In general, output feature maps&#39; shapes can be inferred as follows:</span>

<span class="sd">    input: :math:`(N, C_{\text{in}}, H_{\text{in}}, W_{\text{in}})`</span>

<span class="sd">    output: :math:`(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})` where</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{H}_{out} = \lfloor \frac{\text{H}_{in} + 2 * \text{padding[0]} - </span>
<span class="sd">        \text{dilation[0]} * (\text{kernel_size[0]} - 1) - 1}{\text{stride[0]}} + 1 \rfloor</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{W}_{out} = \lfloor \frac{\text{W}_{in} + 2 * \text{padding[1]} - </span>
<span class="sd">        \text{dilation[1]} * (\text{kernel_size[1]} - 1) - 1}{\text{stride[1]}} + 1 \rfloor</span>

<span class="sd">    When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="sd">    where K is a positive integer, this operation is also known as depthwise</span>
<span class="sd">    convolution.</span>

<span class="sd">    In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    a depthwise convolution with a depthwise multiplier `K`, can be constructed</span>
<span class="sd">    by arguments :math:`(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})`.</span>

<span class="sd">    :param in_channels: number of input channels.</span>
<span class="sd">    :param out_channels: number of output channels.</span>
<span class="sd">    :param kernel_size: size of weight on spatial dimensions. If kernel_size is</span>
<span class="sd">        an :class:`int`, the actual kernel size would be</span>
<span class="sd">        `(kernel_size, kernel_size)`. Default: 1</span>
<span class="sd">    :param stride: stride of the 2D convolution operation. Default: 1</span>
<span class="sd">    :param padding: size of the paddings added to the input on both sides of its</span>
<span class="sd">        spatial dimensions. Only zero-padding is supported. Default: 0</span>
<span class="sd">    :param dilation: dilation of the 2D convolution operation. Default: 1</span>
<span class="sd">    :param groups: number of groups into which the input and output channels are divided,</span>
<span class="sd">        so as to perform a &quot;grouped convolution&quot;. When ``groups`` is not 1,</span>
<span class="sd">        ``in_channels`` and ``out_channels`` must be divisible by ``groups``,</span>
<span class="sd">        and there would be an extra dimension at the beginning of the weight&#39;s</span>
<span class="sd">        shape. Specifically, the shape of weight would be `(groups,</span>
<span class="sd">        out_channel // groups, in_channels // groups, *kernel_size)`.</span>
<span class="sd">    :param bias: whether to add a bias onto the result of convolution. Default:</span>
<span class="sd">        True</span>
<span class="sd">    :param conv_mode: Supports `CROSS_CORRELATION`. Default:</span>
<span class="sd">        `CROSS_CORRELATION`</span>
<span class="sd">    :param compute_mode: When set to &quot;DEFAULT&quot;, no special requirements will be</span>
<span class="sd">        placed on the precision of intermediate results. When set to &quot;FLOAT32&quot;,</span>
<span class="sd">        &quot;Float32&quot; would be used for accumulator and intermediate result, but only</span>
<span class="sd">        effective when input and output are of float16 dtype.</span>

<span class="sd">    Examples:</span>

<span class="sd">    .. testcode::</span>

<span class="sd">        import numpy as np</span>
<span class="sd">        import megengine as mge</span>
<span class="sd">        import megengine.module as M</span>

<span class="sd">        m = M.Conv2d(in_channels=3, out_channels=1, kernel_size=3)</span>
<span class="sd">        inp = mge.tensor(np.arange(0, 96).astype(&quot;float32&quot;).reshape(2, 3, 4, 4))</span>
<span class="sd">        oup = m(inp)</span>
<span class="sd">        print(oup.numpy().shape)</span>

<span class="sd">    Outputs:</span>

<span class="sd">    .. testoutput::</span>

<span class="sd">        (2, 1, 2, 2)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">conv_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CROSS_CORRELATION&quot;</span><span class="p">,</span>
        <span class="n">compute_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;DEFAULT&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair_nonzero</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair_nonzero</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair_nonzero</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span> <span class="o">=</span> <span class="n">conv_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_mode</span> <span class="o">=</span> <span class="n">compute_mode</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_fanin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="n">ic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="k">return</span> <span class="n">kh</span> <span class="o">*</span> <span class="n">kw</span> <span class="o">*</span> <span class="n">ic</span>

    <span class="k">def</span> <span class="nf">_infer_weight_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
        <span class="n">ichl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="n">ochl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span>
        <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="k">if</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Assume format is NCHW</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">ochl</span><span class="p">,</span> <span class="n">ichl</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">ichl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ochl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;invalid config: input_channels=</span><span class="si">{}</span><span class="s2"> output_channels=</span><span class="si">{}</span><span class="s2"> group=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">ichl</span><span class="p">,</span> <span class="n">ochl</span><span class="p">,</span> <span class="n">group</span>
        <span class="p">)</span>
        <span class="c1"># Assume format is NCHW</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">ochl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">ichl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_bias_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Assume format is NCHW</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">calc_conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">conv2d</span><span class="p">(</span>
            <span class="n">inp</span><span class="p">,</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_mode</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">calc_conv</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conv3d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>

    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D convolution over an input tensor.</span>

<span class="sd">    For instance, given an input of the size :math:`(N, C_{\text{in}}, T, H, W)`,</span>
<span class="sd">    this layer generates an output of the size</span>
<span class="sd">    :math:`(N, C_{\text{out}}, T_{\text{out}}}, H_{\text{out}}}, W_{\text{out}}})` through the</span>
<span class="sd">    process described as below:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)</span>

<span class="sd">    where :math:`\star` is the valid 3D cross-correlation operator,</span>
<span class="sd">    :math:`N` is batch size, :math:`C` denotes number of channels</span>


<span class="sd">    When `groups == in_channels` and `out_channels == K * in_channels`,</span>
<span class="sd">    where K is a positive integer, this operation is also known as depthwise</span>
<span class="sd">    convolution.</span>

<span class="sd">    In other words, for an input of size :math:`(N, C_{in}, T_{int}, H_{in}, W_{in})`,</span>
<span class="sd">    a depthwise convolution with a depthwise multiplier `K`, can be constructed</span>
<span class="sd">    by arguments :math:`(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})`.</span>

<span class="sd">    :param in_channels: number of input channels.</span>
<span class="sd">    :param out_channels: number of output channels.</span>
<span class="sd">    :param kernel_size: size of weight on spatial dimensions. If kernel_size is</span>
<span class="sd">        an :class:`int`, the actual kernel size would be</span>
<span class="sd">        `(kernel_size, kernel_size, kernel_size)`. Default: 1</span>
<span class="sd">    :param stride: stride of the 3D convolution operation. Default: 1</span>
<span class="sd">    :param padding: size of the paddings added to the input on both sides of its</span>
<span class="sd">        spatial dimensions. Only zero-padding is supported. Default: 0</span>
<span class="sd">    :param dilation: dilation of the 3D convolution operation. Default: 1</span>
<span class="sd">    :param groups: number of groups into which the input and output channels are divided, so as to perform a &quot;grouped convolution&quot;. When ``groups`` is not 1,</span>
<span class="sd">        ``in_channels`` and ``out_channels`` must be divisible by ``groups``,</span>
<span class="sd">        and there would be an extra dimension at the beginning of the weight&#39;s</span>
<span class="sd">        shape. Specifically, the shape of weight would be `(groups,</span>
<span class="sd">        out_channel // groups, in_channels // groups, *kernel_size)`.</span>
<span class="sd">    :param bias: whether to add a bias onto the result of convolution. Default:</span>
<span class="sd">        True</span>
<span class="sd">    :param conv_mode: Supports `CROSS_CORRELATION`. Default:</span>
<span class="sd">        `CROSS_CORRELATION`</span>

<span class="sd">    Examples:</span>

<span class="sd">    .. testcode::</span>

<span class="sd">        import numpy as np</span>
<span class="sd">        import megengine as mge</span>
<span class="sd">        import megengine.module as M</span>

<span class="sd">        m = M.Conv3d(in_channels=3, out_channels=1, kernel_size=3)</span>
<span class="sd">        inp = mge.tensor(np.arange(0, 384).astype(&quot;float32&quot;).reshape(2, 3, 4, 4, 4))</span>
<span class="sd">        oup = m(inp)</span>
<span class="sd">        print(oup.numpy().shape)</span>

<span class="sd">    Outputs:</span>

<span class="sd">    .. testoutput::</span>

<span class="sd">        (2, 1, 2, 2, 2)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">conv_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CROSS_CORRELATION&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple_nonzero</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple_nonzero</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_triple_nonzero</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span> <span class="o">=</span> <span class="n">conv_mode</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_fanin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">kt</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="n">ic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="k">return</span> <span class="n">kt</span> <span class="o">*</span> <span class="n">kh</span> <span class="o">*</span> <span class="n">kw</span> <span class="o">*</span> <span class="n">ic</span>

    <span class="k">def</span> <span class="nf">_infer_weight_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
        <span class="n">ichl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="n">ochl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span>
        <span class="n">kt</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="k">if</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Assume format is NCTHW</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">ochl</span><span class="p">,</span> <span class="n">ichl</span><span class="p">,</span> <span class="n">kt</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">ichl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ochl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;invalid config: input_channels=</span><span class="si">{}</span><span class="s2"> output_channels=</span><span class="si">{}</span><span class="s2"> group=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">ichl</span><span class="p">,</span> <span class="n">ochl</span><span class="p">,</span> <span class="n">group</span>
        <span class="p">)</span>
        <span class="c1"># Assume format is NCTHW</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">ochl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">ichl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">kt</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_bias_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Assume format is NCTHW</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">calc_conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">conv3d</span><span class="p">(</span>
            <span class="n">inp</span><span class="p">,</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">calc_conv</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D transposed convolution over an input tensor.</span>

<span class="sd">    This module is also known as a deconvolution or a fractionally-strided convolution.</span>
<span class="sd">    :class:`ConvTranspose2d` can be seen as the gradient of :class:`Conv2d` operation</span>
<span class="sd">    with respect to its input.</span>

<span class="sd">    Convolution usually reduces the size of input, while transposed convolution works</span>
<span class="sd">    the opposite way, transforming a smaller input to a larger output while preserving the</span>
<span class="sd">    connectivity pattern.</span>

<span class="sd">    :param in_channels: number of input channels.</span>
<span class="sd">    :param out_channels: number of output channels.</span>
<span class="sd">    :param kernel_size: size of weight on spatial dimensions. If ``kernel_size`` is</span>
<span class="sd">        an :class:`int`, the actual kernel size would be</span>
<span class="sd">        ``(kernel_size, kernel_size)``. Default: 1</span>
<span class="sd">    :param stride: stride of the 2D convolution operation. Default: 1</span>
<span class="sd">    :param padding: size of the paddings added to the input on both sides of its</span>
<span class="sd">        spatial dimensions. Only zero-padding is supported. Default: 0</span>
<span class="sd">    :param dilation: dilation of the 2D convolution operation. Default: 1</span>
<span class="sd">    :param groups: number of groups into which the input and output channels are divided,</span>
<span class="sd">        so as to perform a &quot;grouped convolution&quot;. When ``groups`` is not 1,</span>
<span class="sd">        ``in_channels`` and ``out_channels`` must be divisible by ``groups``,</span>
<span class="sd">        and there would be an extra dimension at the beginning of the weight&#39;s</span>
<span class="sd">        shape. Specifically, the shape of weight would be ``(groups,</span>
<span class="sd">        out_channels // groups, in_channels // groups, *kernel_size)``. Default: 1</span>
<span class="sd">    :param bias: wether to add a bias onto the result of convolution. Default:</span>
<span class="sd">        True</span>
<span class="sd">    :param conv_mode: Supports `CROSS_CORRELATION`. Default:</span>
<span class="sd">        `CROSS_CORRELATION`</span>
<span class="sd">    :param compute_mode: When set to &quot;DEFAULT&quot;, no special requirements will be</span>
<span class="sd">        placed on the precision of intermediate results. When set to &quot;FLOAT32&quot;,</span>
<span class="sd">        &quot;Float32&quot; would be used for accumulator and intermediate result, but only</span>
<span class="sd">        effective when input and output are of float16 dtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">conv_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CROSS_CORRELATION&quot;</span><span class="p">,</span>
        <span class="n">compute_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;DEFAULT&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair_nonzero</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair_nonzero</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair_nonzero</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span> <span class="o">=</span> <span class="n">conv_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_mode</span> <span class="o">=</span> <span class="n">compute_mode</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_fanin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="n">oc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span>
        <span class="k">return</span> <span class="n">kh</span> <span class="o">*</span> <span class="n">kw</span> <span class="o">*</span> <span class="n">oc</span>

    <span class="k">def</span> <span class="nf">_infer_weight_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
        <span class="n">ichl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="n">ochl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span>
        <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="k">if</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Assume format is NCHW</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">ichl</span><span class="p">,</span> <span class="n">ochl</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">ichl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ochl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;invalid config: input_channels=</span><span class="si">{}</span><span class="s2"> output_channels=</span><span class="si">{}</span><span class="s2"> group=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">ichl</span><span class="p">,</span> <span class="n">ochl</span><span class="p">,</span> <span class="n">group</span>
        <span class="p">)</span>
        <span class="c1"># Assume format is NCHW</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">ichl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">ochl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_bias_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Assume format is NCHW</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">conv_transpose2d</span><span class="p">(</span>
            <span class="n">inp</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_mode</span><span class="p">,</span>
        <span class="p">)</span>


<div class="viewcode-block" id="LocalConv2d"><a class="viewcode-back" href="../../../reference/api/megengine.module.LocalConv2d.html#megengine.module.LocalConv2d">[文档]</a><span class="k">class</span> <span class="nc">LocalConv2d</span><span class="p">(</span><span class="n">Conv2d</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a spatial convolution with untied kernels over an groupped channeled input 4D tensor.</span>
<span class="sd">    It is also known as the locally connected layer.</span>

<span class="sd">    :param in_channels: number of input channels.</span>
<span class="sd">    :param out_channels: number of output channels.</span>
<span class="sd">    :param input_height: the height of the input images.</span>
<span class="sd">    :param input_width: the width of the input images.</span>
<span class="sd">    :param kernel_size: size of weight on spatial dimensions. If kernel_size is</span>
<span class="sd">        an :class:`int`, the actual kernel size would be</span>
<span class="sd">        `(kernel_size, kernel_size)`. Default: 1</span>
<span class="sd">    :param stride: stride of the 2D convolution operation. Default: 1</span>
<span class="sd">    :param padding: size of the paddings added to the input on both sides of its</span>
<span class="sd">        spatial dimensions. Only zero-padding is supported. Default: 0</span>
<span class="sd">    :param groups: number of groups into which the input and output channels are divided,</span>
<span class="sd">        so as to perform a &quot;grouped convolution&quot;. When ``groups`` is not 1,</span>
<span class="sd">        ``in_channels`` and ``out_channels`` must be divisible by ``groups``.</span>
<span class="sd">        The shape of weight is `(groups, output_height, output_width,</span>
<span class="sd">        in_channels // groups, *kernel_size, out_channels // groups)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">input_height</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">input_width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">conv_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CROSS_CORRELATION&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_height</span> <span class="o">=</span> <span class="n">input_height</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_width</span> <span class="o">=</span> <span class="n">input_width</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_weight_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
        <span class="n">output_height</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_height</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">output_width</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_width</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="c1"># Assume format is NCHW</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">group</span><span class="p">,</span>
            <span class="n">output_height</span><span class="p">,</span>
            <span class="n">output_width</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">local_conv2d</span><span class="p">(</span>
            <span class="n">inp</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="kc">None</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span><span class="p">,</span>
        <span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ConvRelu2d</span><span class="p">(</span><span class="n">Conv2d</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A fused :class:`~.Module` including :class:`~.module.Conv2d` and :func:`~.relu`.</span>
<span class="sd">    Could be replaced with :class:`~.QATModule` version :class:`~.qat.ConvRelu2d` using :func:`~.quantize.quantize_qat`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">calc_conv</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">DeformableConv2d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deformable Convolution.</span>

<span class="sd">    :param in_channels: number of input channels.</span>
<span class="sd">    :param out_channels: number of output channels.</span>
<span class="sd">    :param kernel_size: size of weight on spatial dimensions. If kernel_size is</span>
<span class="sd">        an :class:`int`, the actual kernel size would be</span>
<span class="sd">        `(kernel_size, kernel_size)`. Default: 1</span>
<span class="sd">    :param stride: stride of the 2D convolution operation. Default: 1</span>
<span class="sd">    :param padding: size of the paddings added to the input on both sides of its</span>
<span class="sd">        spatial dimensions. Only zero-padding is supported. Default: 0</span>
<span class="sd">    :param dilation: dilation of the 2D convolution operation. Default: 1</span>
<span class="sd">    :param groups: number of groups into which the input and output channels are divided,</span>
<span class="sd">        so as to perform a &quot;grouped convolution&quot;. When ``groups`` is not 1,</span>
<span class="sd">        ``in_channels`` and ``out_channels`` must be divisible by ``groups``,</span>
<span class="sd">        and there would be an extra dimension at the beginning of the weight&#39;s</span>
<span class="sd">        shape. Specifically, the shape of weight would be `(groups,</span>
<span class="sd">        out_channel // groups, in_channels // groups, *kernel_size)`.</span>
<span class="sd">    :param bias: whether to add a bias onto the result of convolution. Default:</span>
<span class="sd">        True</span>
<span class="sd">    :param conv_mode: Supports `CROSS_CORRELATION`. Default:</span>
<span class="sd">        `CROSS_CORRELATION`</span>
<span class="sd">    :param compute_mode: When set to &quot;DEFAULT&quot;, no special requirements will be</span>
<span class="sd">        placed on the precision of intermediate results. When set to &quot;FLOAT32&quot;,</span>
<span class="sd">        &quot;Float32&quot; would be used for accumulator and intermediate result, but only</span>
<span class="sd">        effective when input and output are of float16 dtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">conv_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;CROSS_CORRELATION&quot;</span><span class="p">,</span>
        <span class="n">compute_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;DEFAULT&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair_nonzero</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair_nonzero</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair_nonzero</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span> <span class="o">=</span> <span class="n">conv_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_mode</span> <span class="o">=</span> <span class="n">compute_mode</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_fanin</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="n">ic</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="k">return</span> <span class="n">kh</span> <span class="o">*</span> <span class="n">kw</span> <span class="o">*</span> <span class="n">ic</span>

    <span class="k">def</span> <span class="nf">_infer_weight_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
        <span class="n">ichl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span>
        <span class="n">ochl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span>
        <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="k">if</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Assume format is NCHW</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">ochl</span><span class="p">,</span> <span class="n">ichl</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span><span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">ichl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">ochl</span> <span class="o">%</span> <span class="n">group</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;invalid config: input_channels=</span><span class="si">{}</span><span class="s2"> output_channels=</span><span class="si">{}</span><span class="s2"> group=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">ichl</span><span class="p">,</span> <span class="n">ochl</span><span class="p">,</span> <span class="n">group</span>
        <span class="p">)</span>
        <span class="c1"># Assume format is NCHW</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">ochl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">ichl</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="n">kh</span><span class="p">,</span> <span class="n">kw</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_bias_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Assume format is NCHW</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">calc_conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">deformable_conv2d</span><span class="p">(</span>
            <span class="n">inp</span><span class="p">,</span>
            <span class="n">weight</span><span class="p">,</span>
            <span class="n">offset</span><span class="p">,</span>
            <span class="n">mask</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">conv_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compute_mode</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">calc_conv</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</pre></div>

              </div>
              
              
          </main>
          

      </div>
    </div>

    
  <script src="../../../_static/js/index.3c6125c0ae68274ddd1b.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2020-2021, The MegEngine Open Source Team.<br/>
        由 <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.3 创建。<br/>
    </p>
  </div>
</footer>
  </body>
</html>