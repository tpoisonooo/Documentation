
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>自动微分（Auto-diff） &#8212; MegEngine 1.3.0 文档</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.93dda2a1e4f2b831d8345b5b3dbee4ea.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3c6125c0ae68274ddd1b.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/translations.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"extensions": ["tex2jax.js"], "jax": ["input/TeX", "output/HTML-CSS"], "tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script src="../_static/js/custom.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="优化器（Optimizer）" href="optimizer.html" />
    <link rel="prev" title="megengine.module.quantized.DequantStub" href="api/megengine.module.quantized.DequantStub.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">


    
      
      <a class="navbar-brand" href="../index.html">
        <img src="../_static/logo.png" class="logo" alt="logo">
      </a>
      
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../getting-started/index.html">
  新手入门
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../user-guide/index.html">
  用户指南
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="index.html">
  API 参考
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../development/index.html">
  开发者指南
 </a>
</li>

        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://discuss.megengine.org.cn/">论坛<i class="fas fa-external-link-alt"></i></a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://megengine.org.cn/">官网<i class="fas fa-external-link-alt"></i></a>
        </li>
        
      </ul>

      <form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="输入搜索文本..." aria-label="输入搜索文本..." autocomplete="off" >
</form>
      <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/MegEngine/MegEngine" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>

      <script type="text/javascript">
  (function () {
    window.versionSwitcher = {
      pageName: "reference/autodiff.html",
      versionJsonUrl: "/doc/version.json",
      enableLocaleSupport: "True" === "True",
      // TODO read from "zh, en"
      allLocales: [
        {
          "locale": "zh",
          "display": "中文"
        },
        {
          "locale": "en",
          "display": "EN"
        }
      ]
    }
  })();
</script>

<ul class="navbar-nav">
  <li class="nav-item dropdown">
    <button id="version-dropdown" class="btn btn-secondary btn-sm dropdown-toggle" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      <!-- placeholder for javascript filling above -->
    </button>
    <div id="version-menu" class="dropdown-menu" style="min-width: 6rem;">
      <!-- placeholder for javascript filling above -->
    </div>
  </li>
  <li class="nav-item">
    <span id="locale-switcher">
      <!-- placeholder for locale switcher -->
    </span>
  </li>
</ul>
      
        </li>
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="data.html">
   数据（Data）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tensor.html">
   张量（Tensor）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="functional.html">
   函数式（Functional）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="module.html">
   模块式（Module）
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   自动微分（Auto-diff）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optimizer.html">
   优化器（Optimizer）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jit.html">
   即时编译（JIT）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="distributed.html">
   分布式训练（Distributed）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="quantization.html">
   量化（Quantization）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="serialization.html">
   序列化（Serialization）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="random.html">
   随机（Random）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hub.html">
   模型中心（Hub）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logger.html">
   日志（Logger）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="device.html">
   设备（Device）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="version.html">
   查询版本信息
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="utils.html">
   Utils 模块
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradmanager">
   梯度管理器（GradManager）
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#function">
   自定义函数（Function）
  </a>
 </li>
</ul>

</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <span class="target" id="module-megengine.autodiff"></span><div class="section" id="auto-diff">
<h1>自动微分（Auto-diff）<a class="headerlink" href="#auto-diff" title="永久链接至标题">¶</a></h1>
<div class="section" id="gradmanager">
<h2>梯度管理器（GradManager）<a class="headerlink" href="#gradmanager" title="永久链接至标题">¶</a></h2>
<dl class="py class">
<dt id="megengine.autodiff.GradManager">
<em class="property"><span class="pre">class</span> </em><code class="sig-name descname"><span class="pre">GradManager</span></code><a class="reference internal" href="../_modules/megengine/autodiff/grad_manager.html#GradManager"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#megengine.autodiff.GradManager" title="永久链接至目标">¶</a></dt>
<dd><p>基类：<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(在 Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>GradManager computes gradients or more generally, vector-Jacobian product, by reverse mode
automatic differentiation (a.k.a. back propagation).</p>
<p>Reverse mode autodiff normally reuses many intermediate tensors for best computation efficiency.
In a read-eval-print-loop (REPL) environment however, it is impossible to known how the user
would take gradients later thus which tensors to keep. To solve this problem, the user must
somehow declare beforehand which gradient could possibly be taken. With GradManager, users are
required to call the <a class="reference internal" href="#megengine.autodiff.GradManager.attach" title="megengine.autodiff.GradManager.attach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">attach</span></code></a> method on a tensor if they want to take gradients with
respect to it later. Furthermore, any computation on a tensor before it is attached is
completely ignored from the autodiff perspective, so <a class="reference internal" href="#megengine.autodiff.GradManager.attach" title="megengine.autodiff.GradManager.attach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">attach</span></code></a> must be called before any
computation that needs differentiation.</p>
<p>For example, the following symbolic differentiation code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">get_x</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">dy</span> <span class="o">=</span> <span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">dx</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span> <span class="c1"># vector-Jacobian product</span>
</pre></div>
</div>
<p>can be rewriten using GradManager for REPL environment as</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">GradManager</span><span class="p">()</span> <span class="k">as</span> <span class="n">gm</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">get_x</span><span class="p">()</span>
    <span class="n">gm</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># must be placed before any computation on x that needs differentiation</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">dy</span> <span class="o">=</span> <span class="n">ones_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">gm</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span> <span class="c1"># doesn&#39;t need x, already known via attach()</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="c1"># backward() saves result to .grad attribute</span>
</pre></div>
</div>
<p>A more realistic example of training a neural network would be like</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gm</span> <span class="o">=</span> <span class="n">GradManager</span><span class="p">()</span>
<span class="n">gm</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">gm</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">gm</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="c1"># gradients w.r.t. parameters is accumulated into their .grad attributes</span>
</pre></div>
</div>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">record()</span></code> and <code class="docutils literal notranslate"><span class="pre">release()</span></code> method instead of <code class="docutils literal notranslate"><span class="pre">with</span></code> context:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gm</span> <span class="o">=</span> <span class="n">GradManager</span><span class="p">()</span>
<span class="n">gm</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">gm</span><span class="o">.</span><span class="n">record</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">gm</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="c1"># backward() will clear recorded history and free resources</span>
    <span class="c1"># call release() if backward() is not called</span>
    <span class="c1"># gm.release()</span>
</pre></div>
</div>
<p>For your convenience, GradManager may (not must) be reused. As shown in the examples, you
only need to attach a tensor once and GradManager will remember it afterwards.
However, a single GradManager can record only one computation history at a time. To run
multiple differentiations simultaneously or perform high order differentiation, create
as many GradManager as you need.</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>Mutable tensors introduce ambiguities when doing symbolic differentiation: which version
of the tensor are we referring to? For attached tensors, GradManager resolves this
ambiguity by “snapshoting” them on first encounter, either on <a class="reference internal" href="#megengine.autodiff.GradManager.record" title="megengine.autodiff.GradManager.record"><code class="xref py py-meth docutils literal notranslate"><span class="pre">record</span></code></a> (or entering
with statement) if tensor is attached before <a class="reference internal" href="#megengine.autodiff.GradManager.record" title="megengine.autodiff.GradManager.record"><code class="xref py py-meth docutils literal notranslate"><span class="pre">record</span></code></a>, or on <a class="reference internal" href="#megengine.autodiff.GradManager.attach" title="megengine.autodiff.GradManager.attach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">attach</span></code></a> if
GradManager is already recording. Attached tensors will then be interpreted as their
snapshotted version for differentiation purpose. The same ambiguity on the first parameter
of <a class="reference internal" href="#megengine.autodiff.GradManager.backward" title="megengine.autodiff.GradManager.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code></a> is simply resolved by using the latest version.</p>
</div>
<p>Typically, in data parallel, we would like to average the gradients across
processes. Users will finally get the averaged gradients if an “AllReduce”
callback is registered as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">megengine.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="n">gm</span> <span class="o">=</span> <span class="n">GradManager</span><span class="p">()</span>
<span class="n">gm</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">callback</span><span class="o">=</span><span class="n">dist</span><span class="o">.</span><span class="n">make_allreduce_cb</span><span class="p">(</span><span class="s2">&quot;MEAN&quot;</span><span class="p">))</span>
</pre></div>
</div>
<dl class="py method">
<dt id="megengine.autodiff.GradManager.attach">
<code class="sig-name descname"><span class="pre">attach</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensors</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/megengine/autodiff/grad_manager.html#GradManager.attach"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#megengine.autodiff.GradManager.attach" title="永久链接至目标">¶</a></dt>
<dd><p>Instruct GradManager to track operations on tensors, so that gradients with respect
to those tensors could be evaluated later.</p>
<p><a class="reference internal" href="#megengine.autodiff.GradManager.attach" title="megengine.autodiff.GradManager.attach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">attach</span></code></a> also accepts a list of callbacks, which will be called with the tensor and
its gradient during <a class="reference internal" href="#megengine.autodiff.GradManager.backward" title="megengine.autodiff.GradManager.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code></a>. The signature of callbacks should look like:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">callback</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">grad</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="c1"># returned grad is passed to subsequent callbacks</span>
    <span class="c1"># and finally accumulated to the .grad attribute of tensor</span>
    <span class="k">return</span> <span class="n">grad</span>
</pre></div>
</div>
</div></blockquote>
<p><a class="reference internal" href="#megengine.autodiff.GradManager.attach" title="megengine.autodiff.GradManager.attach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">attach</span></code></a> calls with overlapping tensors will result in their callbacks concatenated,
independently for each tensor. For example,</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gm</span><span class="o">.</span><span class="n">attach</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="p">])</span>
<span class="n">gm</span><span class="o">.</span><span class="n">attach</span><span class="p">([</span><span class="n">y</span><span class="p">],</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">g</span><span class="p">])</span>
</pre></div>
</div>
</div></blockquote>
<p>is equivalent to</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">gm</span><span class="o">.</span><span class="n">attach</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="p">])</span>
<span class="n">gm</span><span class="o">.</span><span class="n">attach</span><span class="p">([</span><span class="n">y</span><span class="p">],</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">])</span>
</pre></div>
</div>
</div></blockquote>
<p>The effect of <a class="reference internal" href="#megengine.autodiff.GradManager.attach" title="megengine.autodiff.GradManager.attach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">attach</span></code></a> will persist across multiple uses of the GradManager. When
reusing a GradManager, it is likely a mistake to call <a class="reference internal" href="#megengine.autodiff.GradManager.attach" title="megengine.autodiff.GradManager.attach"><code class="xref py py-meth docutils literal notranslate"><span class="pre">attach</span></code></a> on the same set of
tensors and callbacks repeatedly, which may grow the callback list indefinitely.</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>When reusing a GradManager, it is sometimes desirable to attach temporary tensors each
time, e.g. for computing gradients of inputs of a neural network. GradManager tries to
accommodate such usages by holding weak references to attached tensors. Most of the
times, this should be enough to prevent resource leak. Unfortunately, there are still
some pitfalls left:</p>
<blockquote>
<div><ul class="simple">
<li><p>Callbacks should not hold strong references, directly or indirectly, to attached
tensors. Any strong reference, including those from callbacks, will prevent
garbage collection (even by the cycle collector!) of a attached tensor, until
the GradManager object is garbage collected.</p></li>
</ul>
</div></blockquote>
<p>Please also note that GradManager might hold additional strong references to attached
tensors when it is in use. This note only covers potential resource leaks across
multiple uses of a GradManager, which is unrelated to whether resources is timely
released within a single use.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">参数</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(在 Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a>) – tensor or list of tensors to track</p></li>
<li><p><strong>callbacks</strong> – callback or list of callbacks</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="megengine.autodiff.GradManager.backward">
<code class="sig-name descname"><span class="pre">backward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/megengine/autodiff/grad_manager.html#GradManager.backward"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#megengine.autodiff.GradManager.backward" title="永久链接至目标">¶</a></dt>
<dd><p>Compute gradients (or vector-Jacobian product) for all attached tensors, accumulate to
corresponding .grad attribute, and release resources along the way.</p>
<p><a class="reference internal" href="#megengine.autodiff.GradManager.backward" title="megengine.autodiff.GradManager.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code></a> computes the vector-Jacobian product <span class="math notranslate nohighlight">\(dx_j = \sum_{i} dy_i J_{ij}\)</span>
where <span class="math notranslate nohighlight">\(J_{ij} = ∂y_i/∂x_j\)</span> is the Jacobian matrix between vector variables <span class="math notranslate nohighlight">\(y\)</span>
and <span class="math notranslate nohighlight">\(x\)</span>, with all vectors involved represented as a list of tensors, in the sense of
direct sums (or flatten-and-concatenate). <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(dy\)</span> are passed as the first
and second parameter respectively, whereas <span class="math notranslate nohighlight">\(x\)</span> is directly taken from the list of
all attached tensors. The result <span class="math notranslate nohighlight">\(dx\)</span> is also not returned. Instead, it is directly
accumulated into the .grad attribute of matching attached tensors (a.k.a. <span class="math notranslate nohighlight">\(x\)</span>). This
can be done unambiguously since <span class="math notranslate nohighlight">\(dx\)</span> as a list of tensors has the same structure as
<span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(y\)</span> is a scalar and <span class="math notranslate nohighlight">\(dy\)</span> is chosen to be 1, the vector-Jacobian product
yield gradient of <span class="math notranslate nohighlight">\(y\)</span> with repect to <span class="math notranslate nohighlight">\(x\)</span> as a special case. In that case,
you will be able to omit the <span class="math notranslate nohighlight">\(dy\)</span> parameter and <a class="reference internal" href="#megengine.autodiff.GradManager.backward" title="megengine.autodiff.GradManager.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code></a> will automatically
use 1 for it and compute the gradient.</p>
<p><a class="reference internal" href="#megengine.autodiff.GradManager.backward" title="megengine.autodiff.GradManager.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code></a> consumes all resources held by this GradManager and releases them in the
process of this call. When the call successfully finishes, the GradManager will be put back
to an inactive state.</p>
<dl class="field-list simple">
<dt class="field-odd">参数</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y</strong> – tensor or list of tensors</p></li>
<li><p><strong>dy</strong> – tensor or list of tensors. Defaults to 1 if y is scalar</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="megengine.autodiff.GradManager.record">
<code class="sig-name descname"><span class="pre">record</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/megengine/autodiff/grad_manager.html#GradManager.record"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#megengine.autodiff.GradManager.record" title="永久链接至目标">¶</a></dt>
<dd><p>Start recording operations</p>
<p>After this call, you will be able to call <a class="reference internal" href="#megengine.autodiff.GradManager.backward" title="megengine.autodiff.GradManager.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt id="megengine.autodiff.GradManager.release">
<code class="sig-name descname"><span class="pre">release</span></code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/megengine/autodiff/grad_manager.html#GradManager.release"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#megengine.autodiff.GradManager.release" title="永久链接至目标">¶</a></dt>
<dd><p>Stop recording operations and release resources kept for gradient computation</p>
<p>After this call, you will not be able to call <a class="reference internal" href="#megengine.autodiff.GradManager.backward" title="megengine.autodiff.GradManager.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code></a>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="function">
<h2>自定义函数（Function）<a class="headerlink" href="#function" title="永久链接至标题">¶</a></h2>
<dl class="py class">
<dt id="megengine.autodiff.Function">
<em class="property"><span class="pre">class</span> </em><code class="sig-name descname"><span class="pre">Function</span></code><a class="reference internal" href="../_modules/megengine/core/autodiff/grad.html#Function"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#megengine.autodiff.Function" title="永久链接至目标">¶</a></dt>
<dd><p>基类：<code class="xref py py-class docutils literal notranslate"><span class="pre">megengine.core._imperative_rt.ops.PyOpBase</span></code></p>
<p>Defines a block of operations with customizable differentiation.</p>
<p>The computation should be defined in <code class="docutils literal notranslate"><span class="pre">forward</span></code> method, with gradient
computation defined in <code class="docutils literal notranslate"><span class="pre">backward</span></code> method.</p>
<p>Each instance of <code class="docutils literal notranslate"><span class="pre">Function</span></code> should be used only once during forwardding.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span>
        <span class="k">return</span> <span class="n">dy</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt id="megengine.autodiff.Function.forward">
<code class="sig-name descname"><span class="pre">forward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/megengine/core/autodiff/grad.html#Function.forward"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#megengine.autodiff.Function.forward" title="永久链接至目标">¶</a></dt>
<dd><p>Applies operations to <code class="docutils literal notranslate"><span class="pre">inputs</span></code> and returns results. It must be overriden by all subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">参数</dt>
<dd class="field-odd"><p><strong>input</strong> – input tensors.</p>
</dd>
<dt class="field-even">返回</dt>
<dd class="field-even"><p>a tuple of Tensor or a single Tensor.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>This method should return a tuple of Tensor or a single Tensor representing the output
of the function.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="megengine.autodiff.Function.backward">
<code class="sig-name descname"><span class="pre">backward</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">output_grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/megengine/core/autodiff/grad.html#Function.backward"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#megengine.autodiff.Function.backward" title="永久链接至目标">¶</a></dt>
<dd><p>Compute the gradient of the forward function. It must be overriden by all subclasses.</p>
<dl class="field-list simple">
<dt class="field-odd">参数</dt>
<dd class="field-odd"><p><strong>output_grads</strong> – gradients of outputs that are returned by <a class="reference internal" href="#megengine.autodiff.Function.forward" title="megengine.autodiff.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code></a>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>In case when some tensors of outputs are not related to loss function, the corresponding
values in <code class="docutils literal notranslate"><span class="pre">output_grads</span></code> would be <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>This method should return a tuple which containing the gradients of all inputs, in the same order
as the <code class="docutils literal notranslate"><span class="pre">inputs</span></code> argument of <a class="reference internal" href="#megengine.autodiff.Function.forward" title="megengine.autodiff.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code></a> . A <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> could be returned
instead if there is only one input. If users want to stop the propagation of some gradients,
the corresponding returned values should be set <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p>
</div>
</dd></dl>

</dd></dl>

</div>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>

    
  <script src="../_static/js/index.3c6125c0ae68274ddd1b.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2020-2021, The MegEngine Open Source Team.<br/>
        由 <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.3 创建。<br/>
    </p>
  </div>
</footer>
  </body>
</html>