
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>模型正确性、速度验证与性能调试 &#8212; MegEngine 1.2.0 文档</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.361b90cc13c3b373e3e2df043d87d1bd.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.4bc14ce28f4cf826ca84.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/translations.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <link rel="shortcut icon" href="../_static/megengine-48.png"/>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="使用 codegen 减少访存操作" href="codegen.html" />
    <link rel="prev" title="部署你的模型" href="deploy-your-model.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">


    
      
      <a class="navbar-brand" href="../index.html">
        <img src="../_static/logo.png" class="logo" alt="logo">
      </a>
      
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../getting-started/index.html">
  新手入门
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="index.html">
  用户指南
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../reference/index.html">
  API 参考
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../developmet/index.html">
  开发者指南
 </a>
</li>

        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://discuss.megengine.org.cn/">论坛<i class="fas fa-external-link-alt"></i></a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://megengine.org.cn/">官网<i class="fas fa-external-link-alt"></i></a>
        </li>
        
      </ul>


      <form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="输入搜索文本..." aria-label="输入搜索文本..." autocomplete="off" >
</form>
      

      <ul class="navbar-nav">
        
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/MegEngine/MegEngine" target="_blank" rel="noopener">
              <span><i class="fab fa-github-square"></i></span>
            </a>
          </li>
        
        
        
        <li class="version_switcher nav-item dropdown"><script type="text/javascript">
    (function () {

        // TODO: Handle with api.json file to get the meta-data.

        // Select versions that could be switched by user
        var all_versions = {
            'latest': 'v1.2.0',
            'v1.1': 'v1.1.0',
            'v1.0': 'v1.0.0',
        };

        function change_version(url, new_version) {
            var version_regex = /\/(latest|(v\d+\.\d+.\d+))\//;
            return url.replace(version_regex, '/' + new_version + '/');
        }

        function on_switch() {
            var selected = $(this).children('option:selected').attr('value');

            // original url
            var url = window.location.href;
            // changed url
            var new_url = change_version(url, selected);

            if (new_url != url) {
                // check beforehand if url exists, otherwise redirect to the version's start page
                $.ajax({
                    url: new_url,
                    success: function () {
                        window.location.href = new_url;
                    },
                    error: function () {
                        window.location.href = "https://pydata-sphinx-theme.readthedocs.io/en/" + selected;
                    }
                });
            }
        }

        $(document).ready(function () {
            // var version = DOCUMENTATION_OPTIONS.VERSION;
            // Take the first 2 parts of the release (e.g. "3.4.5" -> "3.4")
            // version = version.split('.').slice(0, 2).join('.');

            // fill the current version in the dropdown
            document.getElementById("version-dropdown").innerText = 'latest';

            const getVersionLink = () => {
                return Object.keys(all_versions).map(key => `<button class="dropdown-item">${key}</button>`)
            }
            // fill the version menu
            document.getElementById("version-menu").innerHTML = getVersionLink().join('');

            // bind the changes to this menu to trigger the switching function
            // TODO: Change this to use the dropdown button's on_select() callback function
            $('.version-dropdown select').bind('change', on_switch);
        });
    })();

</script>

<button id="version-dropdown" class="btn btn-secondary btn-sm dropdown-toggle" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
    <!-- placeholder for javascript filling above -->
</button>
<div id="version-menu" class="dropdown-menu" style="min-width: 6rem;">
    <!-- placeholder for javascript filling above -->
</div> 
        </li>
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="nav bd-sidenav">
      <li class="toctree-l2">
 <a class="reference internal" href="distribution.html">
  分布式训练
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="quantization.html">
  量化训练
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="advanced-parameter-optimization.html">
  参数优化进阶配置
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="trace.html">
  动态图转静态图
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="sublinear-memory.html">
  亚线性内存优化
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="dump.html">
  导出序列化模型
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="deploy-your-model.html">
  部署你的模型
 </a>
</li>

<li class="toctree-l2 current active">
 <a class="current reference internal" href="#">
  模型正确性、速度验证与性能调试
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="codegen.html">
  使用 codegen 减少访存操作
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="midout.html">
  使用 midout 进行端上裁剪
 </a>
</li>

    </ul>
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   如何使用 load_and_run
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     模型准备
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     输入准备
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id17">
     编译 load_and_run
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linux-x86-load-and-run">
       linux x86 平台编译 load_and_run
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linux-arm-load-and-run">
       linux 下交叉编译 arm 版本 load_and_run
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     代码执行
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#layout">
       平台相关 layout 优化
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fastrun">
       fastrun 模式
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#winograd">
       如何开 winograd 优化
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id23">
     正确性验证
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#asserteq">
       开启 asserteq 验证正确性
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dump">
       dump 输出结果
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id28">
       dump 每层结果
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id29">
         文本形式对比结果
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#raw">
         raw形式对比结果
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id30">
     性能调优
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="load-and-run">
<span id="id1"></span><h1>模型正确性、速度验证与性能调试<a class="headerlink" href="#load-and-run" title="永久链接至标题">¶</a></h1>
<div class="section" id="id2">
<h2>如何使用 load_and_run<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p>load_and_run 是 MegEngine 中的加载并运行模型的工具，主要用来做模型正确性验证，速度验证及性能调试，源代码在 <a href="#id3"><span class="problematic" id="id4">`</span></a>load-and-run <a href="#id5"><span class="problematic" id="id6">`</span></a>_ 。</p>
<p>load_and_run 有以下功能：</p>
<ol class="arabic simple">
<li><p>编译出对应各个平台的版本，可对比相同模型的速度；</p></li>
<li><p>测试验证不同模型优化方法的效果，直接执行 ./load_and_run 可得到对应的帮助文档；</p></li>
<li><p><a href="#id7"><span class="problematic" id="id8">`</span></a>dump_with_testcase_mge.py <a href="#id9"><span class="problematic" id="id10">`</span></a>_ 会把输入数据、运行脚本时计算出的结果都打包到模型里，便于比较相同模型在不同平台下的计算结果差异；</p></li>
<li><p>同时支持 <code class="docutils literal notranslate"><span class="pre">--input</span></code> 选项直接设置 mge C++ 模型的输入，输入格式支持 .ppm/.pgm/.json/.npy 等文件格式和命令行。</p></li>
</ol>
<div class="section" id="id11">
<h3>模型准备<a class="headerlink" href="#id11" title="永久链接至标题">¶</a></h3>
<p>将 mge 模型序列化并导出到文件, 我们以 <a href="#id12"><span class="problematic" id="id13">`</span></a>ResNet50 <a href="#id14"><span class="problematic" id="id15">`</span></a>_ 为例。
因为 MegEngine 的模型训练都是动态图形式 ，所以我们需要先将模型转成静态图然后再部署。</p>
<p>具体可参考如下代码片段:</p>
<p><em>代码片段:</em></p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

 <span class="kn">import</span> <span class="nn">megengine.functional</span> <span class="k">as</span> <span class="nn">F</span>
 <span class="kn">import</span> <span class="nn">megengine.hub</span>
 <span class="kn">from</span> <span class="nn">megengine</span> <span class="kn">import</span> <span class="n">jit</span><span class="p">,</span> <span class="n">tensor</span>

 <span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
     <span class="n">net</span> <span class="o">=</span> <span class="n">megengine</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;megengine/models&quot;</span><span class="p">,</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
     <span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

     <span class="nd">@jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">symbolic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">capture_as_const</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
     <span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
         <span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
         <span class="n">pred_normalized</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
         <span class="k">return</span> <span class="n">pred_normalized</span>

     <span class="n">data</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

     <span class="n">fun</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">net</span><span class="o">=</span><span class="n">net</span><span class="p">)</span>
     <span class="n">fun</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="s2">&quot;resnet50.mge&quot;</span><span class="p">,</span> <span class="n">arg_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>
</pre></div>
</td></tr></table></div>
<p>执行脚本，并完成模型转换后，我们就获得了 MegEngine C++ API 可识别的预训练模型文件 <code class="docutils literal notranslate"><span class="pre">resnet50.mge</span></code>。</p>
</div>
<div class="section" id="id16">
<h3>输入准备<a class="headerlink" href="#id16" title="永久链接至标题">¶</a></h3>
<p>load_and_run 可以用 <code class="docutils literal notranslate"><span class="pre">--input</span></code> 选项直接设置模型文件的输入, 它支持 .ppm/.pgm/.json/.npy 等多种格式</p>
<p>测试输入图片如下:</p>
<div class="figure align-default" id="id31">
<img alt="user-guide/fig/cat.jpg" src="user-guide/fig/cat.jpg" />
<p class="caption"><span class="caption-text">图1 猫</span><a class="headerlink" href="#id31" title="永久链接至图片">¶</a></p>
</div>
<p>因为模型的输入是 float32, 且是 nchw, 需要先将图片转成 npy 格式。</p>
<div class="highlight-python notranslate"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span>
<span class="normal">8</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">cat</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;./cat.jpg&#39;</span><span class="p">)</span>
<span class="n">cat</span> <span class="o">=</span> <span class="n">cat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>  <span class="c1"># 将cat的shape从(224,224,3) 变成 (1, 224, 224, 3)</span>
<span class="n">cat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">cat</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># nhwc -&gt; nchw</span>

<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;cat.npy&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">cat</span><span class="p">))</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="id17">
<h3>编译 load_and_run<a class="headerlink" href="#id17" title="永久链接至标题">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>目前发布的版本我们开放了对 cpu（x86, x64, arm, armv8.2）和 gpu（cuda）平台的支持。</p>
</div>
<p>我们在这里以 x86 和 arm 交叉编译为例，来阐述一下如何编译一个 x86 和 arm 的 load_and_run。</p>
<div class="section" id="linux-x86-load-and-run">
<h4>linux x86 平台编译 load_and_run<a class="headerlink" href="#linux-x86-load-and-run" title="永久链接至标题">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/MegEngine/MegEngine.git
<span class="nb">cd</span> MegEngine <span class="o">&amp;&amp;</span> mkdir build <span class="o">&amp;&amp;</span> <span class="nb">cd</span> build
cmake .. -DMGE_WITH_CUDA<span class="o">=</span>OFF -DMGE_WITH_TEST<span class="o">=</span>OFF
make -j<span class="k">$(</span>nproc<span class="k">)</span>
</pre></div>
</div>
<p>编译完成后，我们可以在 <code class="docutils literal notranslate"><span class="pre">build/sdk/load_and_run</span></code> 目录找到 <code class="docutils literal notranslate"><span class="pre">load_and_run</span></code> 。</p>
</div>
<div class="section" id="linux-arm-load-and-run">
<h4>linux 下交叉编译 arm 版本 load_and_run<a class="headerlink" href="#linux-arm-load-and-run" title="永久链接至标题">¶</a></h4>
<p>在 ubuntu(16.04/18.04) 上进行 arm-android 的交叉编译:</p>
<ol class="arabic simple">
<li><p>到 android 的官网下载 ndk 的相关工具，这里推荐 <em>android-ndk-r21</em> 以上的版本：<a class="reference external" href="https://developer.android.google.cn/ndk/downloads/">https://developer.android.google.cn/ndk/downloads/</a></p></li>
<li><p>在 bash 中设置 NDK_ROOT 环境变量：<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">NDK_ROOT=ndk_dir</span></code></p></li>
<li><p>使用以下脚本进行 arm-android 的交叉编译</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./scripts/cmake-build/cross_build_android_arm_inference.sh
</pre></div>
</div>
<p>编译完成后，我们可以在 <code class="docutils literal notranslate"><span class="pre">build_dir/android/arm64-v8a/release/install/bin/load_and_run</span></code> 目录下找到编译生成的可执行文件 <code class="docutils literal notranslate"><span class="pre">load_and_run</span></code>。
默认没有开启 armv8.2-a+dotprod 的新指令集支持，如果在一些支持的设备，如 cortex-a76 等设备，可以开启相关选项(更多选项开关，可以直接看该脚本文件)。</p>
<p>开启 armv8.2-a+dotprod 的代码如下:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./scripts/cmake-build/cross_build_android_arm_inference.sh -p
</pre></div>
</div>
</div>
</div>
<div class="section" id="id18">
<h3>代码执行<a class="headerlink" href="#id18" title="永久链接至标题">¶</a></h3>
<p>下面的实验是在某 android 平台，未开启 armv8.2 指令集(当前测试模型为 float 模型，量化模型推荐开启 armv8.2+dotprod 支持，能够充分利用 dotprod 指令集硬件加速)。</p>
<p>用 <code class="docutils literal notranslate"><span class="pre">load_and_run</span></code> 加载之前 dump 好的 <code class="docutils literal notranslate"><span class="pre">resnet50.mge</span></code> 模型，可以看到类似这样的输出：</p>
<p>先将模型和 load_and_run (依赖 megengine.so )传到手机。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb push build_dir/android/arm64-v8a/release/install/bin/load_and_run /data/local/tmp
adb push build_dir/android/arm64-v8a/release/install/lib/libmegengine.so /data/local/tmp
adb push cat.npy /data/local/tmp
adb push resnet50.mge /data/local/tmp
adb shell <span class="o">&amp;&amp;</span> <span class="nb">cd</span> /data/local/tmp/ <span class="o">&amp;&amp;</span> <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>.:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>之后直接在手机上运行 load_and_run， 可以得到如下输出:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span>
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
load model: <span class="m">198</span>.030ms
<span class="o">===</span> prepare: <span class="m">5</span>.846ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">581</span>.284ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">245</span>.185ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">10</span>.574,device<span class="o">=</span><span class="m">242</span>.226<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">236</span>.910ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.375,device<span class="o">=</span><span class="m">235</span>.615<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">236</span>.811ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.777,device<span class="o">=</span><span class="m">235</span>.569<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">236</span>.921ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.638,device<span class="o">=</span><span class="m">236</span>.340<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">236</span>.321ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.228,device<span class="o">=</span><span class="m">235</span>.713<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">236</span>.975ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.939,device<span class="o">=</span><span class="m">235</span>.407<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">237</span>.215ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.980,device<span class="o">=</span><span class="m">236</span>.614<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">236</span>.335ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.429,device<span class="o">=</span><span class="m">235</span>.867<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">236</span>.702ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.322,device<span class="o">=</span><span class="m">235</span>.440<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">236</span>.964ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.605,device<span class="o">=</span><span class="m">235</span>.727<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2376.339ms avg_time=237.634ms sd=2.668ms minmax=236.321,245.185</span>
</pre></div>
</div>
<div class="section" id="layout">
<h4>平台相关 layout 优化<a class="headerlink" href="#layout" title="永久链接至标题">¶</a></h4>
<p>目前 MegEngine 的网络是 nchw 的 layout，但是这种 layout 不利于充分利用 simd 特性，且边界处理异常复杂。
为此，我们针对 arm 开发了 nchw44 的 layout。</p>
<p>这个命名主要是针对 conv 来定的。</p>
<ol class="arabic simple">
<li><p>nchw: conv 的 feature map 为 (n, c, h, w), weights 为 (oc, ic, fh, fw)。</p></li>
<li><p>nchw44: conv 的 feature map 为 (n, c/4, h, w, 4), weights 为 (oc/4, ic/4, fh, fw, 4(ic), 4(oc))。</p></li>
</ol>
<p>这里从 channel 上取 4 个数排成连续主要方便利用 neon 优化，由于 neon 指令是 128 bit，刚好是 4 个 32 bit，所以定义 nchw44，对于 x86 avx 下，我们同样定义了 nchw88 的 layout 优化。</p>
<p>下面是开启 nchw44 的优化后的结果:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --enable-nchw44
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:26:10 from_argv@mgblar.cpp:1169<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> nchw44 optimization
load model: <span class="m">198</span>.758ms
<span class="o">===</span> prepare: <span class="m">893</span>.954ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">470</span>.390ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">234</span>.949ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.705,device<span class="o">=</span><span class="m">232</span>.806<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">221</span>.953ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.086,device<span class="o">=</span><span class="m">220</span>.651<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">221</span>.841ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.098,device<span class="o">=</span><span class="m">220</span>.585<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">221</span>.968ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.292,device<span class="o">=</span><span class="m">220</span>.742<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">222</span>.159ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.778,device<span class="o">=</span><span class="m">221</span>.564<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">222</span>.377ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.143,device<span class="o">=</span><span class="m">221</span>.772<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">221</span>.741ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.135,device<span class="o">=</span><span class="m">220</span>.662<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">221</span>.947ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.554,device<span class="o">=</span><span class="m">220</span>.948<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">221</span>.934ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.903,device<span class="o">=</span><span class="m">221</span>.352<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">222</span>.711ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.715,device<span class="o">=</span><span class="m">222</span>.109<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2233.580ms avg_time=223.358ms sd=4.083ms minmax=221.741,234.949</span>
</pre></div>
</div>
</div>
<div class="section" id="fastrun">
<h4>fastrun 模式<a class="headerlink" href="#fastrun" title="永久链接至标题">¶</a></h4>
<p>目前在 MegEngine 中，针对某些 opr，尤其是 conv ，存在很多种不同的算法，如 direct, winograd, 或者 im2col 等。这些算法在不同的 shape 或者不同的硬件平台上，其性能表现差别极大，导致很难写出一个有效的搜索算法，在执行时选择到最快的执行方式。为此，我们 MegEngine 集成了 fastrun 模式，也就是在执行模型的时候会将每个 opr 的可选所有算法都执行一遍，然后选择一个最优的算法记录下来。</p>
<p>一般分为两个阶段，搜参和运行。</p>
<ol class="arabic simple">
<li><p>搜参阶段: 开启 fastrun 模式，同时将输出的结果存储到一个 cache 文件中</p></li>
<li><p>执行阶段: 带上 cache 再次执行</p></li>
</ol>
<p>搜参阶段:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --enable-nchw44 --fast-run --fast-run-algo-policy resnet50.cache
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:29:26 from_argv@mgblar.cpp:1169<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> nchw44 optimization
load model: <span class="m">64</span>.370ms
<span class="o">===</span> prepare: <span class="m">846</span>.677ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">1801</span>.133ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">202</span>.185ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.958,device<span class="o">=</span><span class="m">199</span>.600<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">201</span>.051ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.358,device<span class="o">=</span><span class="m">200</span>.491<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">200</span>.205ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.023,device<span class="o">=</span><span class="m">199</span>.627<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">200</span>.640ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.314,device<span class="o">=</span><span class="m">199</span>.393<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">200</span>.506ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.382,device<span class="o">=</span><span class="m">199</span>.376<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">200</span>.918ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.129,device<span class="o">=</span><span class="m">200</span>.333<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">200</span>.342ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.318,device<span class="o">=</span><span class="m">199</span>.750<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">200</span>.487ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.301,device<span class="o">=</span><span class="m">199</span>.287<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">200</span>.326ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.306,device<span class="o">=</span><span class="m">199</span>.290<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">201</span>.089ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.454,device<span class="o">=</span><span class="m">200</span>.511<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2007.749ms avg_time=200.775ms sd=0.584ms minmax=200.205,202.185</span>
</pre></div>
</div>
<p>执行阶段:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --enable-nchw44 --fast-run-algo-policy resnet50.cache
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:29:35 from_argv@mgblar.cpp:1169<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> nchw44 optimization
load model: <span class="m">63</span>.780ms
<span class="o">===</span> prepare: <span class="m">966</span>.115ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">370</span>.681ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">201</span>.882ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.648,device<span class="o">=</span><span class="m">199</span>.450<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">200</span>.812ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.324,device<span class="o">=</span><span class="m">199</span>.593<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">200</span>.328ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.318,device<span class="o">=</span><span class="m">199</span>.737<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">201</span>.167ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.063,device<span class="o">=</span><span class="m">200</span>.566<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">200</span>.554ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.368,device<span class="o">=</span><span class="m">199</span>.398<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">200</span>.783ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.401,device<span class="o">=</span><span class="m">199</span>.536<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">200</span>.631ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.419,device<span class="o">=</span><span class="m">200</span>.037<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">200</span>.824ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.481,device<span class="o">=</span><span class="m">200</span>.493<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">200</span>.972ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.220,device<span class="o">=</span><span class="m">199</span>.852<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">200</span>.210ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.295,device<span class="o">=</span><span class="m">199</span>.351<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2008.163ms avg_time=200.816ms sd=0.471ms minmax=200.210,201.882</span>
</pre></div>
</div>
<p>整体来讲 fastrun 大概有10%的性能提速。</p>
</div>
<div class="section" id="winograd">
<h4>如何开 winograd 优化<a class="headerlink" href="#winograd" title="永久链接至标题">¶</a></h4>
<p>winograd 在 channel 较大的时候，能够有效提升卷积的计算速度，核心思想是加法换乘法。详细原理参考 <a href="#id19"><span class="problematic" id="id20">`</span></a>fast algorithms for convolutional neural networks <a href="#id21"><span class="problematic" id="id22">`</span></a>_。
其在 ResNet 或者 VGG16 等网络, winograd 有非常大的加速效果。</p>
<p>因为对于 3x3 的卷积，有多种 winograd 算法，如 f(2,3), f(4,3), f(6,3)，从理论加速比来讲，f(6,3) &gt; f(4,3) &gt; f(2,3)，
但是 f(6, 3) 的预处理开销更大，因为 MegEngine 内部是基于分块来处理的，feature map 比较小的情况下，f(6,3) 可能会引入比较多的冗余计算，导致其性能不如 f(2,3)，所以可将 winograd 变换和 fastrun 模式结合，基于 fastrun 模式搜索的结果来决定做哪种 winograd 变换。</p>
<p>具体命令如下:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --enable-nchw44 --fast-run --winograd-transform --fast-run-algo-policy resnet50.cache
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:32:52 from_argv@mgblar.cpp:1169<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> nchw44 optimization
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:32:52 from_argv@mgblar.cpp:1394<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> winograd transform
load model: <span class="m">65</span>.021ms
<span class="o">===</span> prepare: <span class="m">1084</span>.991ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">382</span>.357ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">182</span>.904ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.767,device<span class="o">=</span><span class="m">180</span>.191<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">175</span>.491ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">3</span>.972,device<span class="o">=</span><span class="m">174</span>.429<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">175</span>.804ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.193,device<span class="o">=</span><span class="m">174</span>.548<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">176</span>.097ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.383,device<span class="o">=</span><span class="m">175</span>.536<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">175</span>.351ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.200,device<span class="o">=</span><span class="m">174</span>.775<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">175</span>.728ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.525,device<span class="o">=</span><span class="m">174</span>.517<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">175</span>.770ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.052,device<span class="o">=</span><span class="m">174</span>.541<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">175</span>.740ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.251,device<span class="o">=</span><span class="m">175</span>.568<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">175</span>.170ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">3</span>.938,device<span class="o">=</span><span class="m">174</span>.595<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">175</span>.630ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.216,device<span class="o">=</span><span class="m">174</span>.409<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=1763.685ms avg_time=176.368ms sd=2.311ms minmax=175.170,182.904</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id23">
<h3>正确性验证<a class="headerlink" href="#id23" title="永久链接至标题">¶</a></h3>
<p>MegEngine 内置了多种正确性验证的方法，方便检查网络计算正确性。</p>
<div class="section" id="asserteq">
<h4>开启 asserteq 验证正确性<a class="headerlink" href="#asserteq" title="永久链接至标题">¶</a></h4>
<p>可以基于脚本 <a href="#id24"><span class="problematic" id="id25">`</span></a>dump_with_testcase_mge.py <a href="#id26"><span class="problematic" id="id27">`</span></a>_ 将输入数据和运行脚本时使用当前默认的计算设备计算出的模型结果都打包到模型里， 这样在不同平台下就方便比较结果差异了。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 <span class="nv">$MGE</span>/sdk/load_and_run/dump_with_testcase_mge.py ./resnet50.mge --optimize -d cat.jpg -o resnet50.mdl
</pre></div>
</div>
<p>在执行 load_and_run 的时候就不需要再带上 <code class="docutils literal notranslate"><span class="pre">--input</span></code>，因为输入已经打包进 <code class="docutils literal notranslate"><span class="pre">resnet50.mdl</span></code>, 同时在执行 <code class="docutils literal notranslate"><span class="pre">dump_with_testcase_mge.py</span></code> 脚本的时候，会在 xpu (如果有 gpu，就在 gpu 上执行，如果没有就在 cpu 上执行)执行整个网络，将结果作为 <code class="docutils literal notranslate"><span class="pre">ground-truth</span></code> 写入模型中。</p>
<p>我们在执行 load_and_run 的时候会看到:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mdl --iter <span class="m">10</span>
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
load model: <span class="m">81</span>.173ms
<span class="o">===</span> going to run <span class="m">1</span> testcases<span class="p">;</span> output vars: assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">11077</span><span class="o">]{}</span>
<span class="o">===</span> prepare: <span class="m">1</span>.395ms<span class="p">;</span> going to warmup
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
warmup <span class="m">0</span>: <span class="m">544</span>.946ms
<span class="o">===</span> going to run <span class="nb">test</span> <span class="c1">#0 for 10 times</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">0</span>/10: <span class="m">243</span>.277ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">243</span>.267,device<span class="o">=</span><span class="m">241</span>.128<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">241</span>.532ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.522,device<span class="o">=</span><span class="m">241</span>.458<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">240</span>.386ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">240</span>.376,device<span class="o">=</span><span class="m">240</span>.315<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">242</span>.542ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.900,device<span class="o">=</span><span class="m">242</span>.481<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">241</span>.534ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">240</span>.890,device<span class="o">=</span><span class="m">241</span>.476<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">241</span>.036ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.025,device<span class="o">=</span><span class="m">240</span>.965<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">241</span>.657ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.013,device<span class="o">=</span><span class="m">241</span>.596<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">241</span>.663ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.653,device<span class="o">=</span><span class="m">241</span>.594<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">241</span>.520ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.510,device<span class="o">=</span><span class="m">241</span>.448<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">241</span>.766ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.111,device<span class="o">=</span><span class="m">241</span>.704<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2416.913ms avg_time=241.691ms sd=0.779ms minmax=240.386,243.277</span>

<span class="o">===</span> total time: <span class="m">2416</span>.913ms
</pre></div>
</div>
<p>可以看到最大误差是 3.86273e-05.</p>
</div>
<div class="section" id="dump">
<h4>dump 输出结果<a class="headerlink" href="#dump" title="永久链接至标题">¶</a></h4>
<p>同时，我们可以使用 <code class="docutils literal notranslate"><span class="pre">--bin-out-dump</span></code> 在指定的文件夹内保存输出结果。这样就可以用 load-and-run 在目标设备上跑数据集了：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir out
./load_and_run ./resnet50.mge --input ./cat.npy --iter <span class="m">2</span> --bin-out-dump out
</pre></div>
</div>
<p>然后可以在 python 里打开输出文件：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="k">in</span> <span class="o">[</span><span class="m">21</span><span class="o">]</span>: import megengine as mge

<span class="k">in</span> <span class="o">[</span><span class="m">22</span><span class="o">]</span>: <span class="nv">v0</span> <span class="o">=</span> mge.utils.load_tensor_binary<span class="o">(</span><span class="s1">&#39;out/run0-var1602&#39;</span><span class="o">)</span>

<span class="k">in</span> <span class="o">[</span><span class="m">23</span><span class="o">]</span>: <span class="nv">v1</span> <span class="o">=</span> mge.utils.load_tensor_binary<span class="o">(</span><span class="s1">&#39;out/run1-var1602&#39;</span><span class="o">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id28">
<h4>dump 每层结果<a class="headerlink" href="#id28" title="永久链接至标题">¶</a></h4>
<p>我们很多时候会遇到这种情况，就是模型输出结果不对，这个时候就需要打出网络每一层的结果作比对，看看是哪一层导致。目前有两中展现方式，一个是 io-dump, 另一个是 bin-io-dump.</p>
<p>为了对比结果，需要假定一个平台结果为 <code class="docutils literal notranslate"><span class="pre">ground-truth</span></code> ，下面假定以x86的结果为 <code class="docutils literal notranslate"><span class="pre">ground-truth</span></code> ，验证 x86 和 cuda 上的误差产生的原因（下面会使用 <code class="docutils literal notranslate"><span class="pre">host_build.sh</span></code> 编译出来的 <code class="docutils literal notranslate"><span class="pre">load_and_run</span></code> 来演示）。</p>
<div class="section" id="id29">
<h5>文本形式对比结果<a class="headerlink" href="#id29" title="永久链接至标题">¶</a></h5>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --cpu --io-dump cpu.txt
./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --io-dump cuda.txt <span class="c1"># 默认跑在cuda上</span>
vimdiff cpu.txt cuda.txt
</pre></div>
</div>
<p>文档形式只是显示了部分信息，比如 tensor 的前几个输出结果，整个 tensor 的平均值，标准差之类的，如果需要具体到哪个值错误，需要用 bin-io-dump 会将每一层的结果都输出到一个文件。</p>
</div>
<div class="section" id="raw">
<h5>raw形式对比结果<a class="headerlink" href="#raw" title="永久链接至标题">¶</a></h5>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir cpu <span class="o">&amp;&amp;</span> mkdir cuda
./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --cpu --bin-io-dump cpu
./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --bin-io-dump cuda
<span class="nv">$mge</span>/tools/compare_binary_iodump.py cpu cuda
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id30">
<h3>性能调优<a class="headerlink" href="#id30" title="永久链接至标题">¶</a></h3>
<p>load-and-run 可以进行 profiling 并产生一个 json 文件：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --profile model.json
</pre></div>
</div>
<p>这个 model.json 文件可以后续用于 profile_analyze.py 分析。</p>
<p>profile_analyze.py 的示例用法：</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># MGE_ROOT 是 MegEngine 的安装目录</span>
<span class="nv">MGE_ROOT</span><span class="o">=</span><span class="sb">`</span>python3 -c <span class="s2">&quot;import os; \</span>
<span class="s2">                    import megengine; \</span>
<span class="s2">                    print(os.path.dirname(megengine.__file__))&quot;</span><span class="sb">`</span>
<span class="c1"># 输出详细帮助信息</span>
python3 <span class="nv">$MGE_ROOT</span>/utils/profile_analyze.py -h

<span class="c1"># 输出前 5 慢的算子</span>
python3 <span class="nv">$MGE_ROOT</span>/utils/profile_analyze.py ./profiling.json -t <span class="m">5</span>

<span class="c1"># 输出总耗时前 5 大的算子的类型</span>
python3 <span class="nv">$MGE_ROOT</span>/utils/profile_analyze.py ./profiling.json -t <span class="m">5</span> --aggregate-by <span class="nb">type</span> --aggregate sum

<span class="c1"># 按 memory 排序输出用时超过 0.1ms 的 ConvolutionForward 算子</span>
python3 <span class="nv">$MGE_ROOT</span>/utils/profile_analyze.py ./profiling.json -t <span class="m">5</span> --order-by memory --min-time 1e-4  --type ConvolutionForward
</pre></div>
</div>
</div></blockquote>
<p>示例输出：</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt; python3 <span class="nv">$MGE_ROOT</span>/imperative/python/megengine/utils/profile_analyze.py ./model.json -t <span class="m">5</span>
-----------------  ---------
total device <span class="nb">time</span>  <span class="m">0</span>.0118007
total host <span class="nb">time</span>    <span class="m">0</span>.012106
-----------------  ---------

╒════════════════════╤══════════════╤════════════════════════════════╤═══════════════╤═════════╤══════════╤═════════════╤═════════════════╤═══════════════╕
│ device self <span class="nb">time</span>   │ cumulative   │ operator info                  │ computation   │ FLOPS   │ memory   │ bandwidth   │ in_shapes       │ out_shapes    │
╞════════════════════╪══════════════╪════════════════════════════════╪═══════════════╪═════════╪══════════╪═════════════╪═════════════════╪═══════════════╡
│ <span class="c1">#0                 │ 0.000383     │ conv(FUSE_ADD_RELU[351],multi_ │ 231.21        │ 604.00  │ 9.48     │ 24.18       │ {1,512,14,14}   │ {1,512,7,7}   │</span>
│ <span class="m">0</span>.000383           │ <span class="m">3</span>.2%         │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o89<span class="o">)[</span><span class="m">353</span><span class="o">]</span>             │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">512</span>,512,3,3<span class="o">}</span>   │               │
│ <span class="m">3</span>.2%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">353</span>                            │               │         │          │             │                 │               │
├────────────────────┼──────────────┼────────────────────────────────┼───────────────┼─────────┼──────────┼─────────────┼─────────────────┼───────────────┤
│ <span class="c1">#1                 │ 0.000697     │ conv(FUSE_ADD_RELU[383],multi_ │ 102.76        │ 327.08  │ 4.48     │ 13.92       │ {1,2048,7,7}    │ {1,512,7,7}   │</span>
│ <span class="m">0</span>.000314           │ <span class="m">5</span>.9%         │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o100<span class="o">)[</span><span class="m">385</span><span class="o">]</span>            │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">512</span>,2048,1,1<span class="o">}</span>  │               │
│ <span class="m">2</span>.7%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">385</span>                            │               │         │          │             │                 │               │
├────────────────────┼──────────────┼────────────────────────────────┼───────────────┼─────────┼──────────┼─────────────┼─────────────────┼───────────────┤
│ <span class="c1">#2                 │ 0.000949     │ conv(FUSE_ADD_RELU[246],multi_ │ 231.21        │ 917.84  │ 3.21     │ 12.43       │ {1,256,28,28}   │ {1,256,14,14} │</span>
│ <span class="m">0</span>.000252           │ <span class="m">8</span>.0%         │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o59<span class="o">)[</span><span class="m">248</span><span class="o">]</span>             │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">256</span>,256,3,3<span class="o">}</span>   │               │
│ <span class="m">2</span>.1%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">248</span>                            │               │         │          │             │                 │               │
├────────────────────┼──────────────┼────────────────────────────────┼───────────────┼─────────┼──────────┼─────────────┼─────────────────┼───────────────┤
│ <span class="c1">#3                 │ 0.00119      │ conv(FUSE_ADD_RELU[366],multi_ │ 102.76        │ 417.64  │ 4.48     │ 17.78       │ {1,2048,7,7}    │ {1,512,7,7}   │</span>
│ <span class="m">0</span>.000246           │ <span class="m">10</span>.1%        │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o95<span class="o">)[</span><span class="m">368</span><span class="o">]</span>             │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">512</span>,2048,1,1<span class="o">}</span>  │               │
│ <span class="m">2</span>.1%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">368</span>                            │               │         │          │             │                 │               │
├────────────────────┼──────────────┼────────────────────────────────┼───────────────┼─────────┼──────────┼─────────────┼─────────────────┼───────────────┤
│ <span class="c1">#4                 │ 0.00143      │ conv(FUSE_ADD_RELU[346],multi_ │ 205.52        │ 881.88  │ 9.15     │ 38.34       │ {1,1024,14,14}  │ {1,2048,7,7}  │</span>
│ <span class="m">0</span>.000233           │ <span class="m">12</span>.1%        │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o91<span class="o">)[</span><span class="m">361</span><span class="o">]</span>             │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">2048</span>,1024,1,1<span class="o">}</span> │               │
│ <span class="m">2</span>.0%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">361</span>                            │               │         │          │             │                 │               │
╘════════════════════╧══════════════╧════════════════════════════════╧═══════════════╧═════════╧══════════╧═════════════╧═════════════════╧═══════════════╛
</pre></div>
</div>
</div></blockquote>
<p>这个表格打印了前五个耗时最多的算子。每列的含义如下：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">self</span> <span class="pre">time</span></code> 是算子在计算设备上（例如 GPU ）的运行时间</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cumulative</span></code> 累加前面所有算子的时间</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">operator</span> <span class="pre">info</span></code> 打印算子的基本信息</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">computation</span></code> 是算子需要的浮点数操作数目</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FLOPS</span></code> 是算子每秒执行的浮点操作数目，由 <code class="docutils literal notranslate"><span class="pre">computation</span></code> 除以 <code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">self</span> <span class="pre">time</span></code> 并转换单位得到</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">memory</span></code> 是算子使用的存储（例如 GPU 显存）大小</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bandwidth</span></code> 是算子的带宽，由 <code class="docutils literal notranslate"><span class="pre">memory</span></code> 除以 <code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">self</span> <span class="pre">time</span></code> 并转换单位得到</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">in_shapes</span></code> 是算子输入张量的形状</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_shapes</span></code> 是算子输出张量的形状</p></li>
</ul>
</div>
</div>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>

    
  <script src="../_static/js/index.4bc14ce28f4cf826ca84.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2020-2021, The MegEngine Open Source Team.<br/>
        由 <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.1 创建。<br/>
    </p>
  </div>
</footer>
  </body>
</html>