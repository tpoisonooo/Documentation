
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>使用 midout 进行端上裁剪 &#8212; MegEngine 1.2.0 文档</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.0e366ba14472447708f75ccb7d8f24a3.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.9ab83e9ee01d4093105a.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/translations.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script src="../_static/js/custom.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="API 参考" href="../reference/index.html" />
    <link rel="prev" title="使用 codegen 减少访存操作" href="codegen.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">


    
      
      <a class="navbar-brand" href="../index.html">
        <img src="../_static/logo.png" class="logo" alt="logo">
      </a>
      
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../getting-started/index.html">
  新手入门
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="index.html">
  用户指南
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../reference/index.html">
  API 参考
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../developmet/index.html">
  开发者指南
 </a>
</li>

        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://discuss.megengine.org.cn/">论坛<i class="fas fa-external-link-alt"></i></a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://megengine.org.cn/">官网<i class="fas fa-external-link-alt"></i></a>
        </li>
        
      </ul>


      <form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="输入搜索文本..." aria-label="输入搜索文本..." autocomplete="off" >
</form>
      

      <ul class="navbar-nav">
        
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/MegEngine/MegEngine" target="_blank" rel="noopener">
              <span><i class="fab fa-github-square"></i></span>
            </a>
          </li>
        
        
        
        <li class="version_switcher nav-item dropdown"><script type="text/javascript">
    (function () {

        // TODO: Handle with api.json file to get the meta-data.

        // Select versions that could be switched by user
        var all_versions = {
            'latest': 'v1.2.0',
            'v1.1': 'v1.1.0',
            'v1.0': 'v1.0.0',
        };

        function change_version(url, new_version) {
            var version_regex = /\/(latest|(v\d+\.\d+.\d+))\//;
            return url.replace(version_regex, '/' + new_version + '/');
        }

        function on_switch() {
            var selected = $(this).children('option:selected').attr('value');

            // original url
            var url = window.location.href;
            // changed url
            var new_url = change_version(url, selected);

            if (new_url != url) {
                // check beforehand if url exists, otherwise redirect to the version's start page
                $.ajax({
                    url: new_url,
                    success: function () {
                        window.location.href = new_url;
                    },
                    error: function () {
                        window.location.href = "https://pydata-sphinx-theme.readthedocs.io/en/" + selected;
                    }
                });
            }
        }

        $(document).ready(function () {
            // var version = DOCUMENTATION_OPTIONS.VERSION;
            // Take the first 2 parts of the release (e.g. "3.4.5" -> "3.4")
            // version = version.split('.').slice(0, 2).join('.');

            // fill the current version in the dropdown
            document.getElementById("version-dropdown").innerText = 'latest';

            const getVersionLink = () => {
                return Object.keys(all_versions).map(key => `<button class="dropdown-item">${key}</button>`)
            }
            // fill the version menu
            document.getElementById("version-menu").innerHTML = getVersionLink().join('');

            // bind the changes to this menu to trigger the switching function
            // TODO: Change this to use the dropdown button's on_select() callback function
            $('.version-dropdown select').bind('change', on_switch);
        });
    })();

</script>

<button id="version-dropdown" class="btn btn-secondary btn-sm dropdown-toggle" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
    <!-- placeholder for javascript filling above -->
</button>
<div id="version-menu" class="dropdown-menu" style="min-width: 6rem;">
    <!-- placeholder for javascript filling above -->
</div> 
        </li>
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="nav bd-sidenav">
      <li class="toctree-l2">
 <a class="reference internal" href="distribution.html">
  分布式训练
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="quantization.html">
  量化训练
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="advanced-parameter-optimization.html">
  参数优化进阶配置
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="trace.html">
  动态图转静态图
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="sublinear-memory.html">
  亚线性内存优化
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="dump.html">
  导出序列化模型
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="deploy-your-model.html">
  部署你的模型
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="load-and-run.html">
  模型正确性、速度验证与性能调试
 </a>
</li>

<li class="toctree-l2">
 <a class="reference internal" href="codegen.html">
  使用 codegen 减少访存操作
 </a>
</li>

<li class="toctree-l2 current active">
 <a class="current reference internal" href="#">
  使用 midout 进行端上裁剪
 </a>
</li>

    </ul>
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#megengine">
   如何在端上裁剪MegEngine库
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#load-and-run">
     编译静态链接的load_and_run
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     裁剪load_and_run
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dumpopr">
       dump模型获得opr类型名称
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#trace">
       执行模型获得trace文件
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id31">
     使用裁剪后的load_and_run
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id36">
     多个模型合并裁剪
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id41">
     编译选项
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id42">
     裁剪基于MegEngine的应用
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="midout">
<span id="id1"></span><h1>使用 midout 进行端上裁剪<a class="headerlink" href="#midout" title="永久链接至标题">¶</a></h1>
<div class="section" id="megengine">
<h2>如何在端上裁剪MegEngine库<a class="headerlink" href="#megengine" title="永久链接至标题">¶</a></h2>
<p><cite>midout `_ 是MegEngine中用来减小生成的二进制文件体积的工具，有助于在空间受限的设备上部署应用。midout通过记录模型推理时用到的opr和执行流，使用if(0)关闭未被记录的代码段后重新编译，利用 `</cite>-flto`` 链接参数，可以大幅度减少静态链接的可执行文件的大小。其原理请参考 <a href="#id2"><span class="problematic" id="id3">`</span></a>midout原理 <a href="#id4"><span class="problematic" id="id5">`</span></a>_。
现在基于MegEngine提供模型验证工具 <a href="#id6"><span class="problematic" id="id7">`</span></a>load-and-run <a href="#id8"><span class="problematic" id="id9">`</span></a>_，展示怎样在某Aarch64架构的Android端上裁剪MegEngine库。</p>
<div class="section" id="load-and-run">
<h3>编译静态链接的load_and_run<a class="headerlink" href="#load-and-run" title="永久链接至标题">¶</a></h3>
<p>端上裁剪MegEngine库需要一个静态连接MegEngine的可执行程序，编译方法详见 <a href="#id10"><span class="problematic" id="id11">`</span></a>load-and-run的编译 <a href="#id12"><span class="problematic" id="id13">`</span></a>_。
稍有不同的是编译时需要先设置load_and_run静态链接MegEngine。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">EXTRA_CMAKE_ARGS</span><span class="o">=</span><span class="s2">&quot;-DBUILD_SHARED_LIBS=OFF&quot;</span>
</pre></div>
</div>
<p>否则，MegEngine会自动编译成动态库。然后执行：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./cross_build_android_arm_inference.sh
</pre></div>
</div>
<p>查看一下load_and_run的大小：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>du ./build_dir/android/arm64-v8a/Release/install/bin/load_and_run
<span class="m">23200</span>
</pre></div>
</div>
<p>此时load_and_run大小超过20MB。load_and_run的执行，请参考 <a href="#id14"><span class="problematic" id="id15">`</span></a>代码执行 <a href="#id16"><span class="problematic" id="id17">`</span></a>_。</p>
</div>
<div class="section" id="id18">
<h3>裁剪load_and_run<a class="headerlink" href="#id18" title="永久链接至标题">¶</a></h3>
<p>MegEngine的裁剪可以从两方面进行：</p>
<p>1、通过opr裁剪。在dump模型时，可以同时将模型用到的opr信息以json文件的形式输出，midout在编译期裁掉没有被模型使用到的所有opr。</p>
<p>2、通过trace流裁剪。运行一次模型推理，根据代码的执行流生成trace文件，通过trace文件，在二次编译时将没有执行的代码段裁剪掉。</p>
<p>整个裁剪过程分为两个步骤。第一步，dump模型，获得模型opr信息；通过一次推理，获得trace文件。第二步，使用MegEngine的头文件生成工具 <a href="#id19"><span class="problematic" id="id20">`</span></a>gen_header_for_bin_reduce.py <a href="#id21"><span class="problematic" id="id22">`</span></a>_ 将opr信息和trace文件作为输入，生成
bin_reduce.h并将该头文件加入编译Release版的应用程序。当然，也可以单独使用模型opr信息或是trace文件来生成bin_reduce.h，单独使用opr信息时，默认保留所有kernel，单独使用trace文件时，默认保留所有opr。</p>
<div class="section" id="dumpopr">
<h4>dump模型获得opr类型名称<a class="headerlink" href="#dumpopr" title="永久链接至标题">¶</a></h4>
<p>一个模型通常不会用到所有的opr，根据模型使用的opr，可以裁掉那些模型没有使用的opr。在转换模型时，我们可以通过如下方式获得模型的opr信息。
使用 <a href="#id23"><span class="problematic" id="id24">`</span></a>dump_with_testcase_mge.py <a href="#id25"><span class="problematic" id="id26">`</span></a>_ 准备模型时，加上–output-strip-info参数。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 sdk/load-and-run/dump_with_testcase_mge.py --optimize-for-inference resnet50.pkl -o resnet50.mge --enable-fuse-conv-bias-nonlinearity --data <span class="s2">&quot;#rand(0,1)&quot;</span> --no-assert --output-strip-info
</pre></div>
</div>
<p>执行完毕后，会生成resnet50.mge和resnet50.mge.json。查看这个json文件，它记录了模型用到的opr名称。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat resnet50.mge.json
<span class="o">{</span><span class="s2">&quot;hash&quot;</span>: <span class="m">238912597679531219</span>, <span class="s2">&quot;dtypes&quot;</span>: <span class="o">[</span><span class="s2">&quot;Byte&quot;</span>, <span class="s2">&quot;Float32&quot;</span>, <span class="s2">&quot;Int32&quot;</span><span class="o">]</span>, <span class="s2">&quot;opr_types&quot;</span>: <span class="o">[</span><span class="s2">&quot;Concat&quot;</span>, <span class="s2">&quot;ConvBiasForward&quot;</span>, <span class="s2">&quot;ConvolutionForward&quot;</span>, <span class="s2">&quot;Elemwise&quot;</span>, <span class="s2">&quot;GetVarShape&quot;</span>, <span class="s2">&quot;Host2DeviceCopy&quot;</span>, <span class="s2">&quot;ImmutableTensor&quot;</span>, <span class="s2">&quot;MatrixMul&quot;</span>, <span class="s2">&quot;MultipleDeviceTensorHolder&quot;</span>, <span class="s2">&quot;PoolingForward&quot;</span>, <span class="s2">&quot;Reshape&quot;</span>, <span class="s2">&quot;Subtensor&quot;</span><span class="o">]</span>, <span class="s2">&quot;elemwise_modes&quot;</span>: <span class="o">[</span><span class="s2">&quot;ADD&quot;</span>, <span class="s2">&quot;FUSE_ADD_RELU&quot;</span><span class="o">]}</span>
</pre></div>
</div>
</div>
<div class="section" id="trace">
<h4>执行模型获得trace文件<a class="headerlink" href="#trace" title="永久链接至标题">¶</a></h4>
<p>基于trace的裁剪需要通过一次推理获得模型的执行trace文件。具体步骤如下：</p>
<p>1、CMake构建时，打开MGE_WITH_MIDOUT_PROFILE开关，编译load_and_run：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">EXTRA_CMAKE_ARGS</span><span class="o">=</span><span class="s2">&quot;-DMGE_WITH_MIDOUT_PROFILE=ON -DBUILD_SHARED_LIBS=OFF&quot;</span>
./cross_build_android_arm_inference.sh -r
</pre></div>
</div>
<p>编译完成后，将build_dir/android/arm64-v8a/Release/install/bin下的load_and_run推至设备并执行：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge
</pre></div>
</div>
<p>得到如下输出：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mgb load-and-run: using MegBrain MegBrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and MegDNN <span class="m">9</span>.3.0
load model: <span class="m">70</span>.888ms
<span class="o">===</span> going to run <span class="m">1</span> testcases<span class="p">;</span> output vars: ADD<span class="o">(</span>reshape<span class="o">[</span><span class="m">2655</span><span class="o">]</span>,reshape<span class="o">[</span><span class="m">2663</span><span class="o">])[</span><span class="m">2665</span><span class="o">]{</span><span class="m">1</span>,1000<span class="o">}</span>
<span class="o">===</span> prepare: <span class="m">4</span>.873ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">877</span>.578ms
<span class="o">===</span> going to run <span class="nb">test</span> <span class="c1">#0 for 10 times</span>
iter <span class="m">0</span>/10: <span class="m">481</span>.445ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">481</span>.436,device<span class="o">=</span><span class="m">480</span>.794<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">481</span>.192ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">481</span>.183,device<span class="o">=</span><span class="m">481</span>.152<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">480</span>.430ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">480</span>.420,device<span class="o">=</span><span class="m">480</span>.389<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">479</span>.593ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">479</span>.585,device<span class="o">=</span><span class="m">479</span>.553<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">479</span>.851ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">479</span>.843,device<span class="o">=</span><span class="m">479</span>.811<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">479</span>.581ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">479</span>.572,device<span class="o">=</span><span class="m">479</span>.541<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">480</span>.174ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">480</span>.165,device<span class="o">=</span><span class="m">480</span>.134<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">479</span>.443ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">479</span>.435,device<span class="o">=</span><span class="m">479</span>.404<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">479</span>.987ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">479</span>.978,device<span class="o">=</span><span class="m">479</span>.948<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">480</span>.637ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">480</span>.628,device<span class="o">=</span><span class="m">480</span>.598<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=4802.333ms avg_time=480.233ms sd=0.688ms minmax=479.443,481.445</span>

<span class="o">===</span> total time: <span class="m">4802</span>.333ms
midout: <span class="m">110</span> items written to midout_trace.20717
</pre></div>
</div>
<p>注意到执行模型后，生成了midout_trace.20717文件，该文件记录了模型在底层执行了哪些kernel。</p>
<p>2、生成bin_recude.h并再次编译load_and_run：</p>
<p>将生成的midout_trace.20717拷贝至本地，使用上文提到的头文件生成工具 <a href="#id27"><span class="problematic" id="id28">`</span></a>gen_header_for_bin_reduce.py <a href="#id29"><span class="problematic" id="id30">`</span></a>_ 生成bin_reduce.h。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 ./tools/gen_header_for_bin_reduce.py resnet50.mge.json midout_trace.20717 -o bin_reduce.h
</pre></div>
</div>
<p>再次编译load_and_run，注意要将bin_reduce.h加入并编译Release版本。设置CMAKE编译选项：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">EXTRA_CMAKE_ARGS</span><span class="o">=</span><span class="s2">&quot;-DMGE_BIN_REDUCE=/absolute/path/to/bin_reduce.h -DBUILD_SHARED_LIBS=OFF&quot;</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./scripts/cmake-build/cross_build_android_arm_inference.sh -r
</pre></div>
</div>
<p>编译完成后，检查load_and_run的大小：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>du build_dir/android/arm64-v8a/release/install/bin/load_and_run
<span class="m">2264</span>
</pre></div>
</div>
<p>此时load_and_run的大小减小到2MB多。推到设备上运行，得到如下输出：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mgb load-and-run: using MegBrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and MegDNN <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">02</span> <span class="m">15</span>:03:11 check_magic@serializer_mdl.cpp:744<span class="o">][</span>WARN<span class="o">]</span> Graph <span class="o">(</span>with <span class="nb">hash</span> <span class="m">10003400899095033006</span><span class="o">)</span> is not among the graphs fed to midout, may caused by midout json is not create by org pkl also to compat <span class="k">for</span> model operation after dump_with_testcase.py
load model: <span class="m">74</span>.208ms
<span class="o">===</span> going to run <span class="m">1</span> testcases<span class="p">;</span> output vars: ADD<span class="o">(</span>reshape<span class="o">[</span><span class="m">2655</span><span class="o">]</span>,reshape<span class="o">[</span><span class="m">2663</span><span class="o">])[</span><span class="m">2665</span><span class="o">]{</span><span class="m">1</span>,1000<span class="o">}</span>
<span class="o">===</span> prepare: <span class="m">1</span>.251ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">377</span>.813ms
<span class="o">===</span> going to run <span class="nb">test</span> <span class="c1">#0 for 10 times</span>
iter <span class="m">0</span>/10: <span class="m">266</span>.996ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">266</span>.993,device<span class="o">=</span><span class="m">266</span>.854<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">266</span>.717ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">266</span>.715,device<span class="o">=</span><span class="m">266</span>.702<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">266</span>.867ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">266</span>.865,device<span class="o">=</span><span class="m">266</span>.855<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">267</span>.172ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">267</span>.171,device<span class="o">=</span><span class="m">267</span>.159<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">266</span>.820ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">266</span>.819,device<span class="o">=</span><span class="m">266</span>.807<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">266</span>.852ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">266</span>.850,device<span class="o">=</span><span class="m">266</span>.838<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">267</span>.376ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">267</span>.374,device<span class="o">=</span><span class="m">267</span>.363<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">267</span>.005ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">267</span>.003,device<span class="o">=</span><span class="m">266</span>.991<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">266</span>.685ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">266</span>.684,device<span class="o">=</span><span class="m">266</span>.671<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">266</span>.767ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">266</span>.766,device<span class="o">=</span><span class="m">266</span>.755<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2669.257ms avg_time=266.926ms sd=0.216ms minmax=266.685,267.376</span>

<span class="o">===</span> total time: <span class="m">2669</span>.257ms
</pre></div>
</div>
<p>可以看到模型依然正常运行，并且运行速度正常。</p>
</div>
</div>
<div class="section" id="id31">
<h3>使用裁剪后的load_and_run<a class="headerlink" href="#id31" title="永久链接至标题">¶</a></h3>
<p>想要裁剪前后的应用能够正常运行，需要保证裁剪前后两次推理使用同样的命令行参数。如果使用上文裁剪的load_and_fun的fast-run功能(详见 <span class="xref std std-ref">how_to_use_load_and_run</span>)。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run resnet50.mge --fast-run --fast-run-algo-policy resnet50.cache
</pre></div>
</div>
<p>可能得到如下输出：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mgb load-and-run: using MegBrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and MegDNN <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">02</span> <span class="m">15</span>:05:50 check_magic@serializer_mdl.cpp:744<span class="o">][</span>WARN<span class="o">]</span> Graph <span class="o">(</span>with <span class="nb">hash</span> <span class="m">10003400899095033006</span><span class="o">)</span> is not among the graphs fed to midout, may caused by midout json is not create by org pkl also to compat <span class="k">for</span> model operation after dump_with_testcase.py
load model: <span class="m">71</span>.927ms
<span class="o">===</span> going to run <span class="m">1</span> testcases<span class="p">;</span> output vars: ADD<span class="o">(</span>reshape<span class="o">[</span><span class="m">2655</span><span class="o">]</span>,reshape<span class="o">[</span><span class="m">2663</span><span class="o">])[</span><span class="m">2665</span><span class="o">]{</span><span class="m">1</span>,1000<span class="o">}</span>
<span class="o">===</span> prepare: <span class="m">1</span>.251ms<span class="p">;</span> going to warmup
Trap
</pre></div>
</div>
<p>这是因为程序运行到了已经被裁剪掉的函数中，未被记录在trace文件中的函数的实现已经被替换成trap()，详见 <a href="#id32"><span class="problematic" id="id33">`</span></a>midout原理 <a href="#id34"><span class="problematic" id="id35">`</span></a>_。如果想要裁剪与fast-run配合使用，需要按如下流程获得trace文件：</p>
<p>1、开启fast-run模式，执行未裁剪的load_and_run获得.cache文件，注意本次执行生成的trace应该被丢弃：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run resnet50.mge --fast-run --fast-run-algo-policy resnet50.cache
</pre></div>
</div>
<p>2、使用.cache文件，执行load_and_run获得trace文件：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run resnet50.mge --fast-run-algo-policy resnet50.cache --winograd-transform
</pre></div>
</div>
<p>3、如上节，将trace文件拷贝回本机，生成bin_reduce.h，再次编译load_and_run并推至设备。</p>
<p>4、使用裁剪后的load_and_run的fast-run功能，执行同2的命令，得到如下输出：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mgb load-and-run: using MegBrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and MegDNN <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">04</span> <span class="m">15</span>:34:18 from_argv@mgblar.cpp:1392<span class="o">][</span>WARN<span class="o">]</span> <span class="nb">enable</span> winograd transform
<span class="o">[</span><span class="m">04</span> <span class="m">15</span>:34:18 check_magic@serializer_mdl.cpp:744<span class="o">][</span>WARN<span class="o">]</span> Graph <span class="o">(</span>with <span class="nb">hash</span> <span class="m">10003400899095033006</span><span class="o">)</span> is not among the graphs fed to midout, may caused by midout json is not create by org pkl also to compat <span class="k">for</span> model operation after dump_with_testcase.py
load model: <span class="m">64</span>.228ms
<span class="o">===</span> going to run <span class="m">1</span> testcases<span class="p">;</span> output vars: ADD<span class="o">(</span>reshape<span class="o">[</span><span class="m">2655</span><span class="o">]</span>,reshape<span class="o">[</span><span class="m">2663</span><span class="o">])[</span><span class="m">2665</span><span class="o">]{</span><span class="m">1</span>,1000<span class="o">}</span>
<span class="o">===</span> prepare: <span class="m">260</span>.058ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">279</span>.550ms
<span class="o">===</span> going to run <span class="nb">test</span> <span class="c1">#0 for 10 times</span>
iter <span class="m">0</span>/10: <span class="m">209</span>.177ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">209</span>.164,device<span class="o">=</span><span class="m">209</span>.031<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">209</span>.010ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">209</span>.008,device<span class="o">=</span><span class="m">208</span>.997<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">209</span>.024ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">209</span>.022,device<span class="o">=</span><span class="m">209</span>.011<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">208</span>.584ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">208</span>.583,device<span class="o">=</span><span class="m">208</span>.573<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">208</span>.669ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">208</span>.667,device<span class="o">=</span><span class="m">208</span>.658<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">208</span>.849ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">208</span>.847,device<span class="o">=</span><span class="m">208</span>.838<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">208</span>.787ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">208</span>.785,device<span class="o">=</span><span class="m">208</span>.774<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">208</span>.703ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">208</span>.701,device<span class="o">=</span><span class="m">208</span>.692<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">208</span>.918ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">208</span>.916,device<span class="o">=</span><span class="m">208</span>.905<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">208</span>.669ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">208</span>.667,device<span class="o">=</span><span class="m">208</span>.656<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2088.390ms avg_time=208.839ms sd=0.191ms minmax=208.584,209.177</span>

<span class="o">===</span> total time: <span class="m">2088</span>.390ms
</pre></div>
</div>
<p>使用其他load_and_run提供的功能也是如此，想要裁剪前后的应用能够正常运行，需要保证裁剪前后两次推理使用同样的命令行参数。</p>
</div>
<div class="section" id="id36">
<h3>多个模型合并裁剪<a class="headerlink" href="#id36" title="永久链接至标题">¶</a></h3>
<p>多个模型的合并裁剪与单个模型流程相同。 <a href="#id37"><span class="problematic" id="id38">`</span></a>gen_header_for_bin_reduce.py <a href="#id39"><span class="problematic" id="id40">`</span></a>_ 接受多个输入。
假设有模型A与模型B。已经获得A.mge.json,B.mge.json以及A.trace,B.trace。执行：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 ./tools/gen_header_for_bin_reduce.py A.mge.json A.trace B.mge.json B.trace -o bin_reduce.h
</pre></div>
</div>
</div>
<div class="section" id="id41">
<h3>编译选项<a class="headerlink" href="#id41" title="永久链接至标题">¶</a></h3>
<p>MegEngine的cmake中有一些开关是默认打开的，它们提供了RTTI、异常抛出等特性，可以在第二次构建时关闭它们，以获得体积更小的load_and_run。它们是：</p>
<blockquote>
<div><p><cite>MGB_WITH_FLATBUFFERS</cite> : FLABUFFERS格式支持</p>
<p><cite>MGE_ENABLE_RTTI</cite> : C++ RTTI特性</p>
<p><cite>MGE_ENABLE_LOGGING</cite> : 日志功能</p>
<p><cite>MGE_ENABLE_EXCEPTIONS</cite> : 异常功能</p>
</div></blockquote>
<p>MegEngine提供一个总开关 <cite>MGE_WITH_MINIMUM_SIZE</cite> 来关闭上述特性。需要注意的是，只有在MGE_BIN_REDUCE被设置时，此开关才会被检查并生效。</p>
</div>
<div class="section" id="id42">
<h3>裁剪基于MegEngine的应用<a class="headerlink" href="#id42" title="永久链接至标题">¶</a></h3>
<p>可以通过如下几种方式集成MegEngine，对应的裁剪方法相差无几：</p>
<p>1、参照 <cite>CMakeLists.txt `_，将应用集成到整个MegEngine的工程。
假设已经将app.cpp集成到MegEngine，那么会编译出静态链接MegEngine的可执行程序 `app</cite>。只需要按照上文中裁剪load_and_run的流程裁剪 <cite>app</cite> 即可。</p>
<p>2、可能一个应用想要通过静态库集成MegEngine。此时需要获得一个裁剪过的libmegengine.a。可以依然使用load_and_run运行模型获得trace文件，生成bin_reduce.h，并二次编译获得裁剪过的libmegengine.a。
此时，用户使用自己编写的构建脚本构建应用程序，并静态链接libmegengine.a，加上链接参数 <code class="docutils literal notranslate"><span class="pre">-flto=full</span></code>。即可得到裁剪过的基于MegEngine的应用。</p>
<p>3、上述流程亦可以用于libmegengine.so的裁剪，但是动态库的裁剪效果远不及静态库。原因在于动态库并不知道某段代码是否会被调用，因此链接器不会进行激进的优化。</p>
</div>
</div>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>

    
  <script src="../_static/js/index.9ab83e9ee01d4093105a.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2020-2021, The MegEngine Open Source Team.<br/>
        由 <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.2 创建。<br/>
    </p>
  </div>
</footer>
  </body>
</html>