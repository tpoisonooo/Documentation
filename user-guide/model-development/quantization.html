
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>量化训练 &#8212; MegEngine 1.3.0 文档</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.9cd52f7647f75bcab1282abf07de7a52.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.c9faddf98927557166f1.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/translations.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script src="../../_static/js/custom.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="模型评估" href="../model-evaluation/index.html" />
    <link rel="prev" title="分布式训练" href="distributed-training.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">


    
      
      <a class="navbar-brand" href="../../index.html">
        <img src="../../_static/logo.png" class="logo" alt="logo">
      </a>
      
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../getting-started/index.html">
  新手入门
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  用户指南
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../reference/index.html">
  API 参考
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../development/index.html">
  开发者指南
 </a>
</li>

        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://discuss.megengine.org.cn/">论坛<i class="fas fa-external-link-alt"></i></a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://megengine.org.cn/">官网<i class="fas fa-external-link-alt"></i></a>
        </li>
        
      </ul>

      <form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="输入搜索文本..." aria-label="输入搜索文本..." autocomplete="off" >
</form>
      <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/MegEngine/MegEngine" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>

      <script type="text/javascript">
  (function () {
    window.versionSwitcher = {
      pageName: "user-guide/model-development/quantization.html",
      versionJsonUrl: "/doc/version.json",
      enableLocaleSupport: "True" === "True",
      // TODO read from "zh, en"
      allLocales: [
        {
          "locale": "zh",
          "display": "中文"
        },
        {
          "locale": "en",
          "display": "EN"
        }
      ]
    }
  })();
</script>

<ul class="navbar-nav">
  <li class="nav-item dropdown">
    <button id="version-dropdown" class="btn btn-secondary btn-sm dropdown-toggle" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      <!-- placeholder for javascript filling above -->
    </button>
    <div id="version-menu" class="dropdown-menu" style="min-width: 6rem;">
      <!-- placeholder for javascript filling above -->
    </div>
  </li>
  <li class="nav-item">
    <span id="locale-switcher">
      <!-- placeholder for locale switcher -->
    </span>
  </li>
</ul>
      
        </li>
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l2">
  <a class="reference internal" href="../faq.html">
   常见问题汇总（FAQ）
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="../install/index.html">
   编译、构建与安装
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="../basic-concepts/index.html">
   基本概念
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="../data-engineering/index.html">
   数据工程
  </a>
 </li>
 <li class="toctree-l2 current active">
  <a class="reference internal" href="index.html">
   模型开发
  </a>
  <ul class="current">
   <li class="toctree-l3">
    <a class="reference internal" href="trace.html">
     动态图转静态图
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="dump.html">
     导出序列化模型
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="sublinear-memory.html">
     亚线性内存优化
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="advanced-parameter-optimization.html">
     参数优化进阶配置
    </a>
   </li>
   <li class="toctree-l3">
    <a class="reference internal" href="distributed-training.html">
     分布式训练
    </a>
   </li>
   <li class="toctree-l3 current active">
    <a class="current reference internal" href="#">
     量化训练
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="../model-evaluation/index.html">
   模型评估
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="../deployment/index.html">
   模型部署
  </a>
 </li>
</ul>
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   整体流程
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   接口介绍
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#qconfig">
     QConfig
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     模型转换模块与相关基类
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   实例讲解
  </a>
 </li>
</ul>

</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="quantization">
<span id="id1"></span><h1>量化训练<a class="headerlink" href="#quantization" title="永久链接至标题">¶</a></h1>
<p>量化指的是将浮点数模型（一般是 32 位浮点数）的权重或激活值用位数更少的数值类型
（比如 8 位整数、16 位浮点数）来近似表示的过程。
量化后的模型会占用更小的存储空间，还能够利用许多硬件平台上的专属算子进行提速。
比如在 MegEngine 中使用 8 位整数来进行量化，相比默认的 32 位浮点数，
模型大小可以减少为 1/4，而运行在特定的设备上其计算速度也能提升为 2-4 倍。</p>
<p>量化的目的是为了追求极致的推理计算速度，为此舍弃了数值表示的精度，直觉上会带来较大的模型掉点，
但是在使用一系列精细的量化处理之后，其掉点可以变得微乎其微，并能支持正常的部署使用。
而且近年来随着专用神经网络加速芯片的兴起，低比特非浮点的运算方式越来越普及，
因此如何把一个 GPU 上训练的浮点数模型转化为低比特的量化模型，就成为了工业界非常关心的话题。</p>
<p>一般来说，得到量化模型的转换过程按代价从低到高可以分为以下 4 种：</p>
<img alt="../../_images/quant_cls.png" class="align-center" src="../../_images/quant_cls.png" />
<ul class="simple">
<li><p>Type1 和 Type2 由于是在模型浮点模型训练之后介入，无需大量训练数据，
故而转换代价更低，被称为后量化（Post Quantization）；</p></li>
<li><p>Type3 和 Type4 则需要在浮点模型训练时就插入一些假量化（FakeQuantize）算子，
模拟计算过程中数值截断后精度降低的情形，故而称为量化感知训练（Quantization Aware Training, QAT）。</p></li>
</ul>
<p>本文主要介绍 Type2 和 Type3 在 MegEngine 中的完整流程。
事实上，除了 Type2 无需进行假量化，两者的整体流程完全一致。</p>
<div class="section" id="id2">
<h2>整体流程<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p>以 Type3 为例，一般以一个训练完毕的浮点模型为起点，称为 Float 模型。
包含假量化算子的用浮点操作来模拟量化过程的新模型，我们称之为 Quantized-Float 模型，或者 QFloat 模型。
可以直接在终端设备上运行的模型，称之为 Quantized 模型，简称 Q 模型。</p>
<p>而三者的精度一般是 <code class="docutils literal notranslate"><span class="pre">Float</span> <span class="pre">&gt;</span> <span class="pre">QFloat</span> <span class="pre">&gt;</span> <span class="pre">Q</span></code> ，故而一般量化算法也就分为两步：</p>
<ul class="simple">
<li><p>拉近 QFloat 和 Q，这样训练阶段的精度可以作为最终 Q 精度的代理指标，这一阶段偏工程；</p></li>
<li><p>拔高 QFloat 逼近 Float，这样就可以将量化模型性能尽可能恢复到 Float 的精度，这一阶段偏算法。</p></li>
</ul>
<p>典型的三种模型在三个阶段的精度变化如下：</p>
<img alt="../../_images/float-qfloat-q.jpg" class="align-center" src="../../_images/float-qfloat-q.jpg" />
<p>对应到具体的 MegEngine 接口中，三阶段如下：</p>
<ol class="arabic simple">
<li><p>基于 <a class="reference internal" href="../../reference/api/megengine.module.Module.html#megengine.module.Module" title="megengine.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> 搭建网络模型，并按照正常的浮点模型方式进行训练；</p></li>
<li><p>使用 <a class="reference internal" href="../../reference/api/megengine.quantization.quantize_qat.html#megengine.quantization.quantize_qat" title="megengine.quantization.quantize_qat"><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize_qat</span></code></a> 将浮点模型转换为 QFloat 模型，
其中可被量化的关键 Module 会被转换为 <a class="reference internal" href="../../reference/api/megengine.module.qat.QATModule.html#megengine.module.qat.QATModule" title="megengine.module.qat.QATModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">QATModule</span></code></a> ，
并基于量化配置 <a class="reference internal" href="../../reference/api/megengine.quantization.QConfig.html#megengine.quantization.QConfig" title="megengine.quantization.QConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">QConfig</span></code></a> 设置好假量化算子和数值统计方式；</p></li>
<li><p>使用 <code class="xref py py-func docutils literal notranslate"><span class="pre">quantize</span></code> 将 QFloat 模型转换为 Q 模型，
对应的 QATModule 则会被转换为 <a class="reference internal" href="../../reference/api/megengine.module.quantized.QuantizedModule.html#megengine.module.quantized.QuantizedModule" title="megengine.module.quantized.QuantizedModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizedModule</span></code></a> ，
此时网络无法再进行训练，网络中的算子都会转换为低比特计算方式，即可用于部署了。</p></li>
</ol>
<p>该流程是 Type3 对应 QAT 的步骤，Type2 对应的后量化则需使用不同 QConfig，
且需使用 evaluation 模式运行 QFloat 模型，而非训练模式。更多细节可以继续阅读下一节详细的接口介绍。</p>
</div>
<div class="section" id="id3">
<h2>接口介绍<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<p>在 MegEngine 中，最上层的接口是配置如何量化的 <a class="reference internal" href="../../reference/api/megengine.quantization.QConfig.html#megengine.quantization.QConfig" title="megengine.quantization.QConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">QConfig</span></code></a>
和模型转换模块里的 <a class="reference internal" href="../../reference/api/megengine.quantization.quantize_qat.html#megengine.quantization.quantize_qat" title="megengine.quantization.quantize_qat"><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize_qat</span></code></a> 与 <code class="xref py py-func docutils literal notranslate"><span class="pre">quantize</span></code> 。</p>
<div class="section" id="qconfig">
<h3>QConfig<a class="headerlink" href="#qconfig" title="永久链接至标题">¶</a></h3>
<p>QConfig 包括了 <a class="reference internal" href="../../reference/api/megengine.quantization.Observer.html#megengine.quantization.Observer" title="megengine.quantization.Observer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Observer</span></code></a> 和 <a class="reference internal" href="../../reference/api/megengine.quantization.FakeQuantize.html#megengine.quantization.FakeQuantize" title="megengine.quantization.FakeQuantize"><code class="xref py py-class docutils literal notranslate"><span class="pre">FakeQuantize</span></code></a> 两部分。
我们知道，对模型转换为低比特量化模型一般分为两步：
一是统计待量化模型中参数和 activation 的数值范围（scale）和零点（zero_point），
二是根据 scale 和 zero_point 将模型转换成指定的数值类型。而为了统计这两个值，我们需要使用 Observer.</p>
<p>Observer 继承自 <a class="reference internal" href="../../reference/api/megengine.module.Module.html#megengine.module.Module" title="megengine.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> ，也会参与网络的前向传播，
但是其 forward 的返回值就是输入，所以不会影响网络的反向梯度传播。
其作用就是在前向时拿到输入的值，并统计其数值范围，并通过 <code class="xref py py-meth docutils literal notranslate"><span class="pre">get_qparams</span></code> 来获取。
所以在搭建网络时把需要统计数值范围的的 Tensor 作为 Observer 的输入即可。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># forward of MinMaxObserver</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_orig</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enabled</span><span class="p">:</span>
        <span class="c1"># stop gradient</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x_orig</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="c1"># find max and min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="o">.</span><span class="n">_reset</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_val</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="o">.</span><span class="n">_reset</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_val</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
    <span class="k">return</span> <span class="n">x_orig</span>
</pre></div>
</div>
<p>另外如果只观察而不模拟量化会导致模型掉点，于是我们需要有 FakeQuantize
来根据 Observer 观察到的数值范围模拟量化时的截断，使得参数在训练时就能提前“适应“这种操作。
FakeQuantize 在前向时会根据传入的 scale 和 zero_point 对输入 Tensor 做模拟量化的操作，
即先做一遍数值转换再转换后的值还原成原类型，如下所示：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fake_quant_tensor</span><span class="p">(</span><span class="n">inp</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">qmin</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">qmax</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">q_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">q_dict</span><span class="p">[</span><span class="s2">&quot;scale&quot;</span><span class="p">]</span>
    <span class="n">zero_point</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">q_dict</span><span class="p">[</span><span class="s2">&quot;mode&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">QuantMode</span><span class="o">.</span><span class="n">ASYMMERTIC</span><span class="p">:</span>
        <span class="n">zero_point</span> <span class="o">=</span> <span class="n">q_dict</span><span class="p">[</span><span class="s2">&quot;zero_point&quot;</span><span class="p">]</span>
    <span class="c1"># Quant</span>
    <span class="n">oup</span> <span class="o">=</span> <span class="n">Round</span><span class="p">()(</span><span class="n">inp</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span> <span class="o">+</span> <span class="n">zero_point</span>
    <span class="c1"># Clip</span>
    <span class="n">oup</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">oup</span><span class="p">,</span> <span class="n">qmin</span><span class="p">),</span> <span class="n">qmax</span><span class="p">)</span>
    <span class="c1"># Dequant</span>
    <span class="n">oup</span> <span class="o">=</span> <span class="p">(</span><span class="n">oup</span> <span class="o">-</span> <span class="n">zero_point</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="k">return</span> <span class="n">oup</span>
</pre></div>
</div>
<p>目前 MegEngine 支持对 weight/activation 两部分的量化，如下所示：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ema_fakequant_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">weight_observer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">MinMaxObserver</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;qint8&quot;</span><span class="p">,</span> <span class="n">narrow_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">act_observer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">ExponentialMovingAverageObserver</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;qint8&quot;</span><span class="p">,</span> <span class="n">narrow_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">weight_fake_quant</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">FakeQuantize</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;qint8&quot;</span><span class="p">,</span> <span class="n">narrow_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">act_fake_quant</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">FakeQuantize</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;qint8&quot;</span><span class="p">,</span> <span class="n">narrow_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>这里使用了两种 Observer 来统计信息，而 FakeQuantize 使用了默认的算子。</p>
<p>如果是后量化，或者说 Calibration，由于无需进行 FakeQuantize，故而其 fake_quant 属性为 None 即可：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">calibration_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">weight_observer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">MinMaxObserver</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;qint8&quot;</span><span class="p">,</span> <span class="n">narrow_range</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">act_observer</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">HistogramObserver</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;qint8&quot;</span><span class="p">,</span> <span class="n">narrow_range</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">weight_fake_quant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">act_fake_quant</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>除了使用在 <code class="xref py py-mod docutils literal notranslate"><span class="pre">Qconfig</span></code> 里提供的预设 QConfig，
也可以根据需要灵活选择 Observer 和 FakeQuantize  实现自己的 QConfig。目前提供的 Observer 包括：</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../reference/api/megengine.quantization.observer.MinMaxObserver.html#megengine.quantization.observer.MinMaxObserver" title="megengine.quantization.observer.MinMaxObserver"><code class="xref py py-class docutils literal notranslate"><span class="pre">MinMaxObserver</span></code></a> ，
使用最简单的算法统计 min/max，对见到的每批数据取 min/max 跟当前存的值比较并替换，
基于 min/max 得到 scale 和 zero_point；</p></li>
<li><p><a class="reference internal" href="../../reference/api/megengine.quantization.observer.ExponentialMovingAverageObserver.html#megengine.quantization.observer.ExponentialMovingAverageObserver" title="megengine.quantization.observer.ExponentialMovingAverageObserver"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExponentialMovingAverageObserver</span></code></a> ，
引入动量的概念，对每批数据的 min/max 与现有 min/max 的加权和跟现有值比较；</p></li>
<li><p><a class="reference internal" href="../../reference/api/megengine.quantization.observer.HistogramObserver.html#megengine.quantization.observer.HistogramObserver" title="megengine.quantization.observer.HistogramObserver"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistogramObserver</span></code></a> ，
更加复杂的基于直方图分布的 min/max 统计算法，且在 forward 时持续更新该分布，
并根据该分布计算得到 scale 和 zero_point。</p></li>
</ul>
<p>对于 FakeQuantize，目前还提供了 <a class="reference internal" href="../../reference/api/megengine.quantization.fake_quant.TQT.html#megengine.quantization.fake_quant.TQT" title="megengine.quantization.fake_quant.TQT"><code class="xref py py-class docutils literal notranslate"><span class="pre">TQT</span></code></a> 算子，
另外还可以继承 <code class="docutils literal notranslate"><span class="pre">_FakeQuant</span></code> 基类实现自定义的假量化算子。</p>
<p>在实际使用过程中，可能需要在训练时让 Observer 统计并更新参数，但是在推理时则停止更新。
Observer 和 FakeQuantize 都支持 <code class="xref py py-meth docutils literal notranslate"><span class="pre">enable</span></code>
和 <code class="xref py py-meth docutils literal notranslate"><span class="pre">disable</span></code> 功能，
且 Observer 会在 <code class="xref py py-meth docutils literal notranslate"><span class="pre">train</span></code>
和 <code class="xref py py-meth docutils literal notranslate"><span class="pre">train</span></code> 时自动分别调用 enable/disable。</p>
<p>所以一般在 Calibration 时，会先执行 <code class="docutils literal notranslate"><span class="pre">net.eval()</span></code> 保证网络的参数不被更新，
然后再执行 :<code class="docutils literal notranslate"><span class="pre">enable_observer(net)</span></code> 来手动开启 Observer 的统计修改功能。</p>
</div>
<div class="section" id="id4">
<h3>模型转换模块与相关基类<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p>QConfig 提供了一系列如何对模型做量化的接口，而要使用这些接口，
需要网络的 Module 能够在 forward 时给参数、activation 加上 Observer 和进行 FakeQuantize.
转换模块的作用就是将模型中的普通 Module 替换为支持这一系列操作的 <a class="reference internal" href="../../reference/api/megengine.module.qat.QATModule.html#megengine.module.qat.QATModule" title="megengine.module.qat.QATModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">QATModule</span></code></a> ，
并能支持进一步替换成无法训练、专用于部署的 <a class="reference internal" href="../../reference/api/megengine.module.quantized.QuantizedModule.html#megengine.module.quantized.QuantizedModule" title="megengine.module.quantized.QuantizedModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizedModule</span></code></a> 。</p>
<p>基于三种基类实现的 Module 是一一对应的关系，通过转换接口可以依次替换为不同实现的同名 Module。
同时考虑到量化与算子融合（Fuse）的高度关联，我们提供了一系列预先融合好的 Module，
比如 <a class="reference internal" href="../../reference/api/megengine.module.ConvRelu2d.html#megengine.module.ConvRelu2d" title="megengine.module.ConvRelu2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvRelu2d</span></code></a> 、 <a class="reference internal" href="../../reference/api/megengine.module.ConvBn2d.html#megengine.module.ConvBn2d" title="megengine.module.ConvBn2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvBn2d</span></code></a> 和 <a class="reference internal" href="../../reference/api/megengine.module.ConvBnRelu2d.html#megengine.module.ConvBnRelu2d" title="megengine.module.ConvBnRelu2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConvBnRelu2d</span></code></a> 等。
除此之外还提供专用于量化的 <a class="reference internal" href="../../reference/api/megengine.module.QuantStub.html#megengine.module.QuantStub" title="megengine.module.QuantStub"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantStub</span></code></a> 、 <a class="reference internal" href="../../reference/api/megengine.module.DequantStub.html#megengine.module.DequantStub" title="megengine.module.DequantStub"><code class="xref py py-class docutils literal notranslate"><span class="pre">DequantStub</span></code></a> 等辅助模块。</p>
<p>转换的原理很简单，就是将父 Module 中可被量化（Quantable）的子 Module 替换为对应的新 Module.
但是有一些 Quantable Module 还包含 Quantable 子 Module，比如 ConvBn 就包含一个 Conv2d 和一个 BatchNorm2d，
转换过程并不会对这些子 Module 进一步转换，原因是父 Module 被替换之后，
其 forward 计算过程已经完全不同了，不会再依赖于这些子 Module。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>如果需要使一部分 Module 及其子 Module 保留 Float 状态，不进行转换，
可以使用 <code class="xref py py-meth docutils literal notranslate"><span class="pre">disable_quantize</span></code> 来处理。</p>
<p>如果网络结构中涉及一些二元及以上的 ElementWise 操作符，比如加法乘法等，
由于多个输入各自的 scale 并不一致，必须使用量化专用的算子，并指定好输出的 scale.
实际使用中只需要把这些操作替换为 <a class="reference internal" href="../../reference/api/megengine.module.Elemwise.html#megengine.module.Elemwise" title="megengine.module.Elemwise"><code class="xref py py-class docutils literal notranslate"><span class="pre">Elemwise</span></code></a> 即可，
比如 <code class="docutils literal notranslate"><span class="pre">self.add_relu</span> <span class="pre">=</span> <span class="pre">Elemwise(&quot;FUSE_ADD_RELU&quot;)</span></code></p>
<p>另外由于转换过程修改了原网络结构，模型保存与加载无法直接适用于转换后的网络，
读取新网络保存的参数时，需要先调用转换接口得到转换后的网络，才能用 load_state_dict 将参数进行加载。</p>
</div>
</div>
</div>
<div class="section" id="id5">
<h2>实例讲解<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>下面我们以 ResNet18 为例来讲解量化的完整流程，完整代码见 <code class="docutils literal notranslate"><span class="pre">MegEngine/Models</span></code> . 主要分为以下几步：</p>
<ol class="arabic simple">
<li><p>修改网络结构，使用已经 Fuse 好的 ConvBn2d、ConvBnRelu2d、ElementWise 代替原先的 Module；</p></li>
<li><p>在正常模式下预训练模型，并在每轮迭代保存网络检查点；</p></li>
<li><p>调用 <a class="reference internal" href="../../reference/api/megengine.quantization.quantize_qat.html#megengine.quantization.quantize_qat" title="megengine.quantization.quantize_qat"><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize_qat</span></code></a> 转换模型，并进行 finetune；</p></li>
<li><p>调用 <code class="xref py py-func docutils literal notranslate"><span class="pre">quantize</span></code> 转换为量化模型，并执行 dump 用于后续模型部署。</p></li>
</ol>
<p>网络结构见 <code class="docutils literal notranslate"><span class="pre">resnet.py</span></code> ，相比惯常写法，我们修改了其中一些子 Module，
将原先单独的 <code class="docutils literal notranslate"><span class="pre">conv</span></code>, <code class="docutils literal notranslate"><span class="pre">bn</span></code>, <code class="docutils literal notranslate"><span class="pre">relu</span></code> 替换为 Fuse 过的 Quantable Module。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">BasicBlock</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn_relu</span> <span class="o">=</span> <span class="n">ConvBnRelu2d</span><span class="p">(</span>
            <span class="n">in_planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn</span> <span class="o">=</span> <span class="n">ConvBn2d</span><span class="p">(</span>
            <span class="n">planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_relu</span> <span class="o">=</span> <span class="n">Elemwise</span><span class="p">(</span><span class="s2">&quot;FUSE_ADD_RELU&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_planes</span> <span class="o">!=</span> <span class="n">planes</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span>
                <span class="n">ConvBn2d</span><span class="p">(</span><span class="n">in_planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn_relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_bn</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">cut</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_relu</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">cut</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>然后对该模型进行若干轮迭代训练，并保存检查点，这里省略细节：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_steps</span><span class="p">):</span>
    <span class="c1"># Linear learning rate decay</span>
    <span class="n">epoch</span> <span class="o">=</span> <span class="n">step</span> <span class="o">//</span> <span class="n">steps_per_epoch</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>

    <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">train_queue</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">))</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int32&quot;</span><span class="p">))</span>

    <span class="n">n</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">loss</span><span class="p">,</span> <span class="n">acc1</span><span class="p">,</span> <span class="n">acc5</span> <span class="o">=</span> <span class="n">train_func</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">gm</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">clear_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>再调用 <a class="reference internal" href="../../reference/api/megengine.quantization.quantize_qat.html#megengine.quantization.quantize_qat" title="megengine.quantization.quantize_qat"><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize_qat</span></code></a> 来将网络转换为 QATModule：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="o">~.</span><span class="n">quantization</span> <span class="kn">import</span> <span class="nn">ema_fakequant_qconfig</span>
<span class="kn">from</span> <span class="o">~.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize</span> <span class="kn">import</span> <span class="nn">quantize_qat</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ResNet18</span><span class="p">()</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
    <span class="n">quantize_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ema_fakequant_qconfig</span><span class="p">)</span>
</pre></div>
</div>
<p>这里使用默认的 <code class="docutils literal notranslate"><span class="pre">ema_fakequant_qconfig</span></code> 来进行 <code class="docutils literal notranslate"><span class="pre">int8</span></code> 量化。</p>
<p>然后我们继续使用上面相同的代码进行 finetune 训练。
值得注意的是，如果这两步全在一次程序运行中执行，那么训练的 trace 函数需要用不一样的，
因为模型的参数变化了，需要重新进行编译。
示例代码中则是采用在新的执行中读取检查点重新编译的方法。</p>
<p>在 QAT 模式训练完成后，我们继续保存检查点，执行 <code class="docutils literal notranslate"><span class="pre">inference.py</span></code> 并设置 <code class="docutils literal notranslate"><span class="pre">mode</span></code> 为 <code class="docutils literal notranslate"><span class="pre">quantized</span></code> ，
这里需要将原始 Float 模型转换为 QAT 模型之后再加载检查点。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="o">~.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize</span> <span class="kn">import</span> <span class="nn">quantize_qat</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ResNet18</span><span class="p">()</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
    <span class="n">quantize_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ema_fakequant_qconfig</span><span class="p">)</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">:</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Load pretrained weights from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">)</span>
    <span class="n">ckpt</span> <span class="o">=</span> <span class="n">mge</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">)</span>
    <span class="n">ckpt</span> <span class="o">=</span> <span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="s2">&quot;state_dict&quot;</span> <span class="ow">in</span> <span class="n">ckpt</span> <span class="k">else</span> <span class="n">ckpt</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>模型转换为量化模型包括以下几步：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="o">~.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize</span> <span class="kn">import</span> <span class="nn">quantize</span>

<span class="c1"># 定义trace函数，打开capture_as_const以进行dump</span>
<span class="nd">@jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">capture_as_const</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">infer_func</span><span class="p">(</span><span class="n">processed_img</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">processed_img</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">probs</span>

<span class="c1"># 执行模型转换</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;quantized&quot;</span><span class="p">:</span>
    <span class="n">quantize</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># 准备数据</span>
<span class="n">processed_img</span> <span class="o">=</span> <span class="n">transform</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">image</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
<span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;normal&quot;</span><span class="p">:</span>
    <span class="n">processed_img</span> <span class="o">=</span> <span class="n">processed_img</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="k">elif</span> <span class="n">args</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;quantized&quot;</span><span class="p">:</span>
    <span class="n">processed_img</span> <span class="o">=</span> <span class="n">processed_img</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int8&quot;</span><span class="p">)</span>

<span class="c1"># 执行一遍evaluation</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">infer_func</span><span class="p">(</span><span class="n">processed_img</span><span class="p">)</span>

<span class="c1"># 将模型 dump 导出</span>
<span class="n">infer_func</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="n">arg_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>至此便得到了一个可用于部署的量化模型。</p>
</div>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>

    
  <script src="../../_static/js/index.c9faddf98927557166f1.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2020-2021, The MegEngine Open Source Team.<br/>
        由 <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.3 创建。<br/>
    </p>
  </div>
</footer>
  </body>
</html>