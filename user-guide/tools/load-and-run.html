
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>如何使用 Load and Run（C++） &#8212; MegEngine 1.3.0 文档</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.93dda2a1e4f2b831d8345b5b3dbee4ea.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3c6125c0ae68274ddd1b.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/translations.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script src="../../_static/js/custom.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="如何使用 Load and Run（Python）" href="load-and-run-py.html" />
    <link rel="prev" title="图手术操作指南" href="graphsurgeon.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">


    
      
      <a class="navbar-brand" href="../../index.html">
        <img src="../../_static/logo.png" class="logo" alt="logo">
      </a>
      
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../getting-started/index.html">
  新手入门
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="../index.html">
  用户指南
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../reference/index.html">
  API 参考
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../development/index.html">
  开发者指南
 </a>
</li>

        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://discuss.megengine.org.cn/">论坛<i class="fas fa-external-link-alt"></i></a>
        </li>
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://megengine.org.cn/">官网<i class="fas fa-external-link-alt"></i></a>
        </li>
        
      </ul>

      <form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="输入搜索文本..." aria-label="输入搜索文本..." autocomplete="off" >
</form>
      <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/MegEngine/MegEngine" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
      </ul>

      <script type="text/javascript">
  (function () {
    window.versionSwitcher = {
      pageName: "user-guide/tools/load-and-run.html",
      versionJsonUrl: "/doc/version.json",
      enableLocaleSupport: "True" === "True",
      // TODO read from "zh, en"
      allLocales: [
        {
          "locale": "zh",
          "display": "中文"
        },
        {
          "locale": "en",
          "display": "EN"
        }
      ]
    }
  })();
</script>

<ul class="navbar-nav">
  <li class="nav-item dropdown">
    <button id="version-dropdown" class="btn btn-secondary btn-sm dropdown-toggle" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      <!-- placeholder for javascript filling above -->
    </button>
    <div id="version-menu" class="dropdown-menu" style="min-width: 6rem;">
      <!-- placeholder for javascript filling above -->
    </div>
  </li>
  <li class="nav-item">
    <span id="locale-switcher">
      <!-- placeholder for locale switcher -->
    </span>
  </li>
</ul>
      
        </li>
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption">
 <span class="caption-text">
  安装
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../install/index.html">
   安装 MegEngine
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  模型开发
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../model-development/advanced-parameter-optimization.html">
   参数优化进阶配置
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../model-development/trace.html">
   将动态图转为静态图
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../model-development/dump.html">
   导出模型序列化文件
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  分布式训练
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../distributed-training.html">
   分布式训练
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  模型压缩
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../model-compression/quantization.html">
   量化
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  模型部署
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../deployment/index.html">
   将模型部署到 C++ 环境
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deployment/midout.html">
   减少二进制文件体积
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  各类工具
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="module-stats.html">
   参数量与计算量统计
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="runtimeopr.html">
   RuntimeOpr 使用说明
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="graphsurgeon.html">
   图手术操作指南
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   如何使用 Load and Run（C++）
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="load-and-run-py.html">
   如何使用 Load and Run（Python）
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   模型准备
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   输入准备
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   编译 load_and_run
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linux-x86">
     linux x86 平台编译
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linux-arm">
     linux 下交叉编译 arm 版本
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   代码执行
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layout">
     平台相关 layout 优化
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fastrun">
     fastrun 模式
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#winograd">
     如何开 winograd 优化
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   正确性验证
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#asserteq">
     开启 asserteq 验证正确性
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dump">
     dump 输出结果
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     dump 每层结果
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   性能调优
  </a>
 </li>
</ul>

</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="load-and-run-c">
<span id="load-and-run"></span><h1>如何使用 Load and Run（C++）<a class="headerlink" href="#load-and-run-c" title="永久链接至标题">¶</a></h1>
<p>Load and Run  是 MegEngine 中的加载并运行模型的工具，主要用来做模型正确性验证，速度验证及性能调试。
它具有以下功能：</p>
<ol class="arabic simple">
<li><p>编译出对应各个平台的二进制文件，可对比相同模型的速度；</p></li>
<li><p>测试验证不同模型优化方法的效果，直接执行 <code class="docutils literal notranslate"><span class="pre">./load_and_run</span></code> 可得到对应的帮助文档；</p></li>
<li><p>支持 <code class="docutils literal notranslate"><span class="pre">--input</span></code> 选项直接设置 mge C++ 模型的输入，输入格式支持 .ppm/.pgm/.json/.npy 等文件格式和命令行。</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>二进制版本体积较大不利于 <code class="docutils literal notranslate"><span class="pre">pip</span></code> 用户使用，可选择使用 Load and Run 的 <a class="reference internal" href="load-and-run-py.html#load-and-run-py"><span class="std std-ref">Python 版本</span></a> 。</p>
</div>
<div class="section" id="id1">
<h2>模型准备<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h2>
<p>将 mge 模型序列化并导出到文件, 我们以 <code class="docutils literal notranslate"><span class="pre">ResNet50</span></code> 为例。
因为 MegEngine 的模型训练都是动态图形式 ，所以我们需要先将模型转成静态图然后再部署。</p>
<p>具体可参考如下代码片段:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">megengine.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">megengine.hub</span>
<span class="kn">from</span> <span class="nn">megengine</span> <span class="kn">import</span> <span class="n">jit</span><span class="p">,</span> <span class="n">tensor</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">megengine</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;megengine/models&quot;</span><span class="p">,</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">net</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="nd">@jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">symbolic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">capture_as_const</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">pred_normalized</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pred_normalized</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="n">fun</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">net</span><span class="o">=</span><span class="n">net</span><span class="p">)</span>
    <span class="n">fun</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="s2">&quot;resnet50.mge&quot;</span><span class="p">,</span> <span class="n">arg_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>执行脚本，并完成模型转换后，我们就获得了 MegEngine C++ API 可识别的预训练模型文件 <code class="docutils literal notranslate"><span class="pre">resnet50.mge</span></code> .</p>
</div>
<div class="section" id="id2">
<h2>输入准备<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<p>load_and_run 可以用 <code class="docutils literal notranslate"><span class="pre">--input</span></code> 选项直接设置模型文件的输入, 它支持 .ppm/.pgm/.json/.npy 等多种格式</p>
<p>测试输入图片如下:</p>
<img alt="../../_images/cat.jpg" src="../../_images/cat.jpg" />
<p>因为模型的输入是 float32, 且是 nchw, 需要先将图片转成 npy 格式。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">cat</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;./cat.jpg&#39;</span><span class="p">)</span>
<span class="n">cat</span> <span class="o">=</span> <span class="n">cat</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>  <span class="c1"># 将cat的shape从(224,224,3) 变成 (1, 224, 224, 3)</span>
<span class="n">cat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">cat</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># nhwc -&gt; nchw</span>

<span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;cat.npy&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">cat</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h2>编译 load_and_run<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>目前发布的版本我们开放了对 cpu（x86, x64, arm, armv8.2）和 gpu（cuda）平台的支持。</p>
</div>
<p>我们在这里以 x86 和 arm 交叉编译为例，来阐述一下如何编译一个 x86 和 arm 的 load_and_run。</p>
<div class="section" id="linux-x86">
<h3>linux x86 平台编译<a class="headerlink" href="#linux-x86" title="永久链接至标题">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/MegEngine/MegEngine.git
<span class="nb">cd</span> MegEngine <span class="o">&amp;&amp;</span> mkdir build <span class="o">&amp;&amp;</span> <span class="nb">cd</span> build
cmake .. -DMGE_WITH_CUDA<span class="o">=</span>OFF -DMGE_WITH_TEST<span class="o">=</span>OFF
make -j<span class="k">$(</span>nproc<span class="k">)</span>
</pre></div>
</div>
<p>编译完成后，我们可以在 <code class="docutils literal notranslate"><span class="pre">build/sdk/load_and_run</span></code> 目录找到 <code class="docutils literal notranslate"><span class="pre">load_and_run</span></code> .</p>
</div>
<div class="section" id="linux-arm">
<h3>linux 下交叉编译 arm 版本<a class="headerlink" href="#linux-arm" title="永久链接至标题">¶</a></h3>
<p>在 ubuntu(16.04/18.04) 上进行 arm-android 的交叉编译:</p>
<ol class="arabic simple">
<li><p>到 android 的官网下载 ndk 的相关工具，这里推荐 android-ndk-r21 以上的版本</p></li>
<li><p>在 bash 中设置 NDK_ROOT 环境变量：<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">NDK_ROOT=ndk_dir</span></code></p></li>
<li><p>使用以下脚本进行 arm-android 的交叉编译</p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./scripts/cmake-build/cross_build_android_arm_inference.sh
</pre></div>
</div>
<p>编译完成后，我们可以在 <code class="docutils literal notranslate"><span class="pre">build_dir/android/arm64-v8a/release/install/bin/load_and_run</span></code>
目录下找到编译生成的可执行文件 <code class="docutils literal notranslate"><span class="pre">load_and_run</span></code> . 默认没有开启 armv8.2-a+dotprod 的新指令集支持，
如果在一些支持的设备，如 cortex-a76 等设备，可以开启相关选项(更多选项开关，可以直接看该脚本文件)。</p>
<p>开启 armv8.2-a+dotprod 的代码如下:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./scripts/cmake-build/cross_build_android_arm_inference.sh -p
</pre></div>
</div>
</div>
</div>
<div class="section" id="id4">
<h2>代码执行<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h2>
<p>下面的实验是在某 android 平台，未开启 armv8.2 指令集(当前测试模型为 float 模型，
量化模型推荐开启 armv8.2+dotprod 支持，能够充分利用 dotprod 指令集硬件加速)。</p>
<p>用 <code class="docutils literal notranslate"><span class="pre">load_and_run</span></code> 加载之前 dump 好的 <code class="docutils literal notranslate"><span class="pre">resnet50.mge</span></code> 模型，可以看到类似这样的输出：</p>
<p>先将模型和 load_and_run (依赖 megengine.so )传到手机。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>adb push build_dir/android/arm64-v8a/release/install/bin/load_and_run /data/local/tmp
adb push build_dir/android/arm64-v8a/release/install/lib/libmegengine.so /data/local/tmp
adb push cat.npy /data/local/tmp
adb push resnet50.mge /data/local/tmp
adb shell <span class="o">&amp;&amp;</span> <span class="nb">cd</span> /data/local/tmp/ <span class="o">&amp;&amp;</span> <span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>.:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>之后直接在手机上运行 load_and_run， 可以得到如下输出:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span>
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
load model: <span class="m">198</span>.030ms
<span class="o">===</span> prepare: <span class="m">5</span>.846ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">581</span>.284ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">245</span>.185ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">10</span>.574,device<span class="o">=</span><span class="m">242</span>.226<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">236</span>.910ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.375,device<span class="o">=</span><span class="m">235</span>.615<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">236</span>.811ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.777,device<span class="o">=</span><span class="m">235</span>.569<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">236</span>.921ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.638,device<span class="o">=</span><span class="m">236</span>.340<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">236</span>.321ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.228,device<span class="o">=</span><span class="m">235</span>.713<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">236</span>.975ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.939,device<span class="o">=</span><span class="m">235</span>.407<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">237</span>.215ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.980,device<span class="o">=</span><span class="m">236</span>.614<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">236</span>.335ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.429,device<span class="o">=</span><span class="m">235</span>.867<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">236</span>.702ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.322,device<span class="o">=</span><span class="m">235</span>.440<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">236</span>.964ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.605,device<span class="o">=</span><span class="m">235</span>.727<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2376.339ms avg_time=237.634ms sd=2.668ms minmax=236.321,245.185</span>
</pre></div>
</div>
<div class="section" id="layout">
<h3>平台相关 layout 优化<a class="headerlink" href="#layout" title="永久链接至标题">¶</a></h3>
<p>目前 MegEngine 的网络是 nchw 的 layout，但是这种 layout 不利于充分利用 simd 特性，且边界处理异常复杂。
为此，我们针对 arm 开发了 nchw44 的 layout。</p>
<p>这个命名主要是针对 conv 来定的。</p>
<ol class="arabic simple">
<li><p>nchw: conv 的 feature map 为 (n, c, h, w), weights 为 (oc, ic, fh, fw)。</p></li>
<li><p>nchw44: conv 的 feature map 为 (n, c/4, h, w, 4), weights 为 (oc/4, ic/4, fh, fw, 4(ic), 4(oc))。</p></li>
</ol>
<p>这里从 channel 上取 4 个数排成连续主要方便利用 neon 优化，
由于 neon 指令是 128 bit，刚好是 4 个 32 bit，所以定义 nchw44，
对于 x86 avx 下，我们同样定义了 nchw88 的 layout 优化。</p>
<p>下面是开启 nchw44 的优化后的结果:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --enable-nchw44
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:26:10 from_argv@mgblar.cpp:1169<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> nchw44 optimization
load model: <span class="m">198</span>.758ms
<span class="o">===</span> prepare: <span class="m">893</span>.954ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">470</span>.390ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">234</span>.949ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">6</span>.705,device<span class="o">=</span><span class="m">232</span>.806<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">221</span>.953ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.086,device<span class="o">=</span><span class="m">220</span>.651<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">221</span>.841ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.098,device<span class="o">=</span><span class="m">220</span>.585<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">221</span>.968ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.292,device<span class="o">=</span><span class="m">220</span>.742<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">222</span>.159ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.778,device<span class="o">=</span><span class="m">221</span>.564<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">222</span>.377ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.143,device<span class="o">=</span><span class="m">221</span>.772<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">221</span>.741ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.135,device<span class="o">=</span><span class="m">220</span>.662<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">221</span>.947ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.554,device<span class="o">=</span><span class="m">220</span>.948<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">221</span>.934ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.903,device<span class="o">=</span><span class="m">221</span>.352<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">222</span>.711ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.715,device<span class="o">=</span><span class="m">222</span>.109<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2233.580ms avg_time=223.358ms sd=4.083ms minmax=221.741,234.949</span>
</pre></div>
</div>
</div>
<div class="section" id="fastrun">
<h3>fastrun 模式<a class="headerlink" href="#fastrun" title="永久链接至标题">¶</a></h3>
<p>目前在 MegEngine 中，针对某些 opr，尤其是 conv ，存在很多种不同的算法，如 direct, winograd, 或者 im2col 等。
这些算法在不同的 shape 或者不同的硬件平台上，其性能表现差别极大，
导致很难写出一个有效的搜索算法，在执行时选择到最快的执行方式。
为此，我们 MegEngine 集成了 fastrun 模式，也就是在执行模型的时候会将每个 opr 的可选所有算法都执行一遍，
然后选择一个最优的算法记录下来。</p>
<p>一般分为两个阶段，搜参和运行。</p>
<ol class="arabic simple">
<li><p>搜参阶段: 开启 fastrun 模式，同时将输出的结果存储到一个 cache 文件中</p></li>
<li><p>执行阶段: 带上 cache 再次执行</p></li>
</ol>
<p>搜参阶段:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --enable-nchw44 --fast-run --fast-run-algo-policy resnet50.cache
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:29:26 from_argv@mgblar.cpp:1169<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> nchw44 optimization
load model: <span class="m">64</span>.370ms
<span class="o">===</span> prepare: <span class="m">846</span>.677ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">1801</span>.133ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">202</span>.185ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.958,device<span class="o">=</span><span class="m">199</span>.600<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">201</span>.051ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.358,device<span class="o">=</span><span class="m">200</span>.491<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">200</span>.205ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.023,device<span class="o">=</span><span class="m">199</span>.627<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">200</span>.640ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.314,device<span class="o">=</span><span class="m">199</span>.393<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">200</span>.506ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.382,device<span class="o">=</span><span class="m">199</span>.376<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">200</span>.918ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.129,device<span class="o">=</span><span class="m">200</span>.333<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">200</span>.342ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.318,device<span class="o">=</span><span class="m">199</span>.750<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">200</span>.487ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.301,device<span class="o">=</span><span class="m">199</span>.287<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">200</span>.326ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.306,device<span class="o">=</span><span class="m">199</span>.290<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">201</span>.089ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.454,device<span class="o">=</span><span class="m">200</span>.511<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2007.749ms avg_time=200.775ms sd=0.584ms minmax=200.205,202.185</span>
</pre></div>
</div>
<p>执行阶段:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --enable-nchw44 --fast-run-algo-policy resnet50.cache
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:29:35 from_argv@mgblar.cpp:1169<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> nchw44 optimization
load model: <span class="m">63</span>.780ms
<span class="o">===</span> prepare: <span class="m">966</span>.115ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">370</span>.681ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">201</span>.882ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.648,device<span class="o">=</span><span class="m">199</span>.450<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">200</span>.812ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.324,device<span class="o">=</span><span class="m">199</span>.593<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">200</span>.328ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.318,device<span class="o">=</span><span class="m">199</span>.737<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">201</span>.167ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.063,device<span class="o">=</span><span class="m">200</span>.566<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">200</span>.554ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.368,device<span class="o">=</span><span class="m">199</span>.398<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">200</span>.783ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.401,device<span class="o">=</span><span class="m">199</span>.536<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">200</span>.631ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.419,device<span class="o">=</span><span class="m">200</span>.037<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">200</span>.824ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.481,device<span class="o">=</span><span class="m">200</span>.493<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">200</span>.972ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.220,device<span class="o">=</span><span class="m">199</span>.852<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">200</span>.210ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.295,device<span class="o">=</span><span class="m">199</span>.351<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2008.163ms avg_time=200.816ms sd=0.471ms minmax=200.210,201.882</span>
</pre></div>
</div>
<p>整体来讲 fastrun 大概有 10% 的性能提速。</p>
</div>
<div class="section" id="winograd">
<h3>如何开 winograd 优化<a class="headerlink" href="#winograd" title="永久链接至标题">¶</a></h3>
<p>winograd 在 channel 较大的时候，能够有效提升卷积的计算速度，核心思想是加法换乘法。
详细原理参考 <a class="reference external" href="https://arxiv.org/pdf/1509.09308.pdf">Fast Algorithms for Convolutional Neural Networks</a> .
其在 ResNet 或者 VGG16 等网络, winograd 有非常大的加速效果。</p>
<p>因为对于 3x3 的卷积，有多种 winograd 算法，如 f(2,3), f(4,3), f(6,3)，从理论加速比来讲，f(6,3) &gt; f(4,3) &gt; f(2,3)，
但是 f(6, 3) 的预处理开销更大，因为 MegEngine 内部是基于分块来处理的，
feature map 比较小的情况下，f(6,3) 可能会引入比较多的冗余计算，导致其性能不如 f(2,3)，
所以可将 winograd 变换和 fastrun 模式结合，基于 fastrun 模式搜索的结果来决定做哪种 winograd 变换。</p>
<p>具体命令如下:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --enable-nchw44 --fast-run --winograd-transform --fast-run-algo-policy resnet50.cache
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:32:52 from_argv@mgblar.cpp:1169<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> nchw44 optimization
<span class="o">[</span><span class="m">19</span> <span class="m">00</span>:32:52 from_argv@mgblar.cpp:1394<span class="o">][</span>warn<span class="o">]</span> <span class="nb">enable</span> winograd transform
load model: <span class="m">65</span>.021ms
<span class="o">===</span> prepare: <span class="m">1084</span>.991ms<span class="p">;</span> going to warmup
warmup <span class="m">0</span>: <span class="m">382</span>.357ms
<span class="o">===</span> going to run input <span class="k">for</span> <span class="m">10</span> <span class="nb">times</span>
iter <span class="m">0</span>/10: <span class="m">182</span>.904ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">5</span>.767,device<span class="o">=</span><span class="m">180</span>.191<span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">175</span>.491ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">3</span>.972,device<span class="o">=</span><span class="m">174</span>.429<span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">175</span>.804ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.193,device<span class="o">=</span><span class="m">174</span>.548<span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">176</span>.097ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.383,device<span class="o">=</span><span class="m">175</span>.536<span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">175</span>.351ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.200,device<span class="o">=</span><span class="m">174</span>.775<span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">175</span>.728ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.525,device<span class="o">=</span><span class="m">174</span>.517<span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">175</span>.770ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.052,device<span class="o">=</span><span class="m">174</span>.541<span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">175</span>.740ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.251,device<span class="o">=</span><span class="m">175</span>.568<span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">175</span>.170ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">3</span>.938,device<span class="o">=</span><span class="m">174</span>.595<span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">175</span>.630ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">4</span>.216,device<span class="o">=</span><span class="m">174</span>.409<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=1763.685ms avg_time=176.368ms sd=2.311ms minmax=175.170,182.904</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id5">
<h2>正确性验证<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>MegEngine 内置了多种正确性验证的方法，方便检查网络计算正确性。</p>
<div class="section" id="asserteq">
<h3>开启 asserteq 验证正确性<a class="headerlink" href="#asserteq" title="永久链接至标题">¶</a></h3>
<p>可以基于脚本 <code class="docutils literal notranslate"><span class="pre">dump_with_testcase_mge.py</span></code> 将输入数据和运行脚本时
使用当前默认的计算设备计算出的模型结果都打包到模型里， 这样在不同平台下就方便比较结果差异了。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 <span class="nv">$MGE</span>/sdk/load_and_run/dump_with_testcase_mge.py ./resnet50.mge --optimize -d cat.jpg -o resnet50.mdl
</pre></div>
</div>
<p>在执行 load_and_run 的时候就不需要再带上 <code class="docutils literal notranslate"><span class="pre">--input</span></code> ，因为输入已经打包进 <code class="docutils literal notranslate"><span class="pre">resnet50.mdl</span></code> ,
同时在执行 <code class="docutils literal notranslate"><span class="pre">dump_with_testcase_mge.py</span></code> 脚本的时候，会在 xpu (如果有 gpu，就在 gpu 上执行，
如果没有就在 cpu 上执行)执行整个网络，将结果作为 <code class="docutils literal notranslate"><span class="pre">ground-truth</span></code> 写入模型中。</p>
<p>我们在执行 load_and_run 的时候会看到:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mdl --iter <span class="m">10</span>
mgb load-and-run: using megbrain <span class="m">8</span>.4.1<span class="o">(</span><span class="m">0</span><span class="o">)</span> and megdnn <span class="m">9</span>.3.0
load model: <span class="m">81</span>.173ms
<span class="o">===</span> going to run <span class="m">1</span> testcases<span class="p">;</span> output vars: assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">11077</span><span class="o">]{}</span>
<span class="o">===</span> prepare: <span class="m">1</span>.395ms<span class="p">;</span> going to warmup
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
warmup <span class="m">0</span>: <span class="m">544</span>.946ms
<span class="o">===</span> going to run <span class="nb">test</span> <span class="c1">#0 for 10 times</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">0</span>/10: <span class="m">243</span>.277ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">243</span>.267,device<span class="o">=</span><span class="m">241</span>.128<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">1</span>/10: <span class="m">241</span>.532ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.522,device<span class="o">=</span><span class="m">241</span>.458<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">2</span>/10: <span class="m">240</span>.386ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">240</span>.376,device<span class="o">=</span><span class="m">240</span>.315<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">3</span>/10: <span class="m">242</span>.542ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.900,device<span class="o">=</span><span class="m">242</span>.481<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">4</span>/10: <span class="m">241</span>.534ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">240</span>.890,device<span class="o">=</span><span class="m">241</span>.476<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">5</span>/10: <span class="m">241</span>.036ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.025,device<span class="o">=</span><span class="m">240</span>.965<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">6</span>/10: <span class="m">241</span>.657ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.013,device<span class="o">=</span><span class="m">241</span>.596<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">7</span>/10: <span class="m">241</span>.663ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.653,device<span class="o">=</span><span class="m">241</span>.594<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">8</span>/10: <span class="m">241</span>.520ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.510,device<span class="o">=</span><span class="m">241</span>.448<span class="o">)</span>
assertequal: <span class="nv">err</span><span class="o">=</span><span class="m">3</span>.86273e-05 <span class="o">(</span><span class="nv">name</span><span class="o">=</span>assert_eq<span class="o">(</span>true_div<span class="o">[</span><span class="m">5741</span><span class="o">]</span>:expect,true_div<span class="o">[</span><span class="m">5741</span><span class="o">])[</span><span class="m">472</span><span class="o">]</span> <span class="nv">id</span><span class="o">=</span><span class="m">472</span><span class="o">)</span>
iter <span class="m">9</span>/10: <span class="m">241</span>.766ms <span class="o">(</span><span class="nv">exec</span><span class="o">=</span><span class="m">241</span>.111,device<span class="o">=</span><span class="m">241</span>.704<span class="o">)</span>
<span class="o">===</span> finished <span class="nb">test</span> <span class="c1">#0: time=2416.913ms avg_time=241.691ms sd=0.779ms minmax=240.386,243.277</span>

<span class="o">===</span> total time: <span class="m">2416</span>.913ms
</pre></div>
</div>
<p>可以看到最大误差是 3.86273e-05.</p>
</div>
<div class="section" id="dump">
<h3>dump 输出结果<a class="headerlink" href="#dump" title="永久链接至标题">¶</a></h3>
<p>同时，我们可以使用 <code class="docutils literal notranslate"><span class="pre">--bin-out-dump</span></code> 在指定的文件夹内保存输出结果。
这样就可以用 load-and-run 在目标设备上跑数据集了：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir out
./load_and_run ./resnet50.mge --input ./cat.npy --iter <span class="m">2</span> --bin-out-dump out
</pre></div>
</div>
<p>然后可以在 python 里打开输出文件：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="k">in</span> <span class="o">[</span><span class="m">21</span><span class="o">]</span>: import megengine as mge

<span class="k">in</span> <span class="o">[</span><span class="m">22</span><span class="o">]</span>: <span class="nv">v0</span> <span class="o">=</span> mge.utils.load_tensor_binary<span class="o">(</span><span class="s1">&#39;out/run0-var1602&#39;</span><span class="o">)</span>

<span class="k">in</span> <span class="o">[</span><span class="m">23</span><span class="o">]</span>: <span class="nv">v1</span> <span class="o">=</span> mge.utils.load_tensor_binary<span class="o">(</span><span class="s1">&#39;out/run1-var1602&#39;</span><span class="o">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h3>dump 每层结果<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<p>我们很多时候会遇到这种情况，就是模型输出结果不对，
这个时候就需要打出网络每一层的结果作比对，看看是哪一层导致。
目前有两中展现方式，一个是 io-dump, 另一个是 bin-io-dump.</p>
<p>为了对比结果，需要假定一个平台结果为 <code class="docutils literal notranslate"><span class="pre">ground-truth</span></code> ，
下面假定以x86的结果为 <code class="docutils literal notranslate"><span class="pre">ground-truth</span></code> ，验证 x86 和 cuda 上的误差产生的原因
（下面会使用 <code class="docutils literal notranslate"><span class="pre">host_build.sh</span></code> 编译出来的 <code class="docutils literal notranslate"><span class="pre">load_and_run</span></code> 来演示）。</p>
<p>文本形式对比结果：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --cpu --io-dump cpu.txt
./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --io-dump cuda.txt <span class="c1"># 默认跑在cuda上</span>
vimdiff cpu.txt cuda.txt
</pre></div>
</div>
<p>文档形式只是显示了部分信息，比如 tensor 的前几个输出结果，整个 tensor 的平均值，标准差之类的，如果需要具体到哪个值错误，需要用 bin-io-dump 会将每一层的结果都输出到一个文件。</p>
<p>raw 形式对比结果：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir cpu <span class="o">&amp;&amp;</span> mkdir cuda
./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --cpu --bin-io-dump cpu
./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --bin-io-dump cuda
<span class="nv">$mge</span>/tools/compare_binary_iodump.py cpu cuda
</pre></div>
</div>
</div>
</div>
<div class="section" id="id7">
<h2>性能调优<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h2>
<p>load-and-run 可以进行 profiling 并产生一个 json 文件：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./load_and_run ./resnet50.mge --input cat.npy --iter <span class="m">10</span> --profile model.json
</pre></div>
</div>
<p>这个 model.json 文件可以后续用于 megengine.utils.profile_analyze 分析。</p>
<p>megengine.utils.profile_analyze 的示例用法：</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 输出详细帮助信息</span>
python3 -m megengine.utils.profile_analyze -h

<span class="c1"># 输出前 5 慢的算子</span>
python3 -m megengine.utils.profile_analyze ./profiling.json -t <span class="m">5</span>

<span class="c1"># 输出总耗时前 5 大的算子的类型</span>
python3 -m megengine.utils.profile_analyze ./profiling.json -t <span class="m">5</span> --aggregate-by <span class="nb">type</span> --aggregate sum

<span class="c1"># 按 memory 排序输出用时超过 0.1ms 的 ConvolutionForward 算子</span>
python3 -m megengine.utils.profile_analyze ./profiling.json -t <span class="m">5</span> --order-by memory --min-time 1e-4  --type ConvolutionForward
</pre></div>
</div>
</div></blockquote>
<p>示例输出：</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt; python3 -m megengine.utils.profile_analyze ./model.json -t <span class="m">5</span>
-----------------  ---------
total device <span class="nb">time</span>  <span class="m">0</span>.0118007
total host <span class="nb">time</span>    <span class="m">0</span>.012106
-----------------  ---------

╒════════════════════╤══════════════╤════════════════════════════════╤═══════════════╤═════════╤══════════╤═════════════╤═════════════════╤═══════════════╕
│ device self <span class="nb">time</span>   │ cumulative   │ operator info                  │ computation   │ FLOPS   │ memory   │ bandwidth   │ in_shapes       │ out_shapes    │
╞════════════════════╪══════════════╪════════════════════════════════╪═══════════════╪═════════╪══════════╪═════════════╪═════════════════╪═══════════════╡
│ <span class="c1">#0                 │ 0.000383     │ conv(FUSE_ADD_RELU[351],multi_ │ 231.21        │ 604.00  │ 9.48     │ 24.18       │ {1,512,14,14}   │ {1,512,7,7}   │</span>
│ <span class="m">0</span>.000383           │ <span class="m">3</span>.2%         │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o89<span class="o">)[</span><span class="m">353</span><span class="o">]</span>             │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">512</span>,512,3,3<span class="o">}</span>   │               │
│ <span class="m">3</span>.2%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">353</span>                            │               │         │          │             │                 │               │
├────────────────────┼──────────────┼────────────────────────────────┼───────────────┼─────────┼──────────┼─────────────┼─────────────────┼───────────────┤
│ <span class="c1">#1                 │ 0.000697     │ conv(FUSE_ADD_RELU[383],multi_ │ 102.76        │ 327.08  │ 4.48     │ 13.92       │ {1,2048,7,7}    │ {1,512,7,7}   │</span>
│ <span class="m">0</span>.000314           │ <span class="m">5</span>.9%         │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o100<span class="o">)[</span><span class="m">385</span><span class="o">]</span>            │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">512</span>,2048,1,1<span class="o">}</span>  │               │
│ <span class="m">2</span>.7%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">385</span>                            │               │         │          │             │                 │               │
├────────────────────┼──────────────┼────────────────────────────────┼───────────────┼─────────┼──────────┼─────────────┼─────────────────┼───────────────┤
│ <span class="c1">#2                 │ 0.000949     │ conv(FUSE_ADD_RELU[246],multi_ │ 231.21        │ 917.84  │ 3.21     │ 12.43       │ {1,256,28,28}   │ {1,256,14,14} │</span>
│ <span class="m">0</span>.000252           │ <span class="m">8</span>.0%         │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o59<span class="o">)[</span><span class="m">248</span><span class="o">]</span>             │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">256</span>,256,3,3<span class="o">}</span>   │               │
│ <span class="m">2</span>.1%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">248</span>                            │               │         │          │             │                 │               │
├────────────────────┼──────────────┼────────────────────────────────┼───────────────┼─────────┼──────────┼─────────────┼─────────────────┼───────────────┤
│ <span class="c1">#3                 │ 0.00119      │ conv(FUSE_ADD_RELU[366],multi_ │ 102.76        │ 417.64  │ 4.48     │ 17.78       │ {1,2048,7,7}    │ {1,512,7,7}   │</span>
│ <span class="m">0</span>.000246           │ <span class="m">10</span>.1%        │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o95<span class="o">)[</span><span class="m">368</span><span class="o">]</span>             │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">512</span>,2048,1,1<span class="o">}</span>  │               │
│ <span class="m">2</span>.1%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">368</span>                            │               │         │          │             │                 │               │
├────────────────────┼──────────────┼────────────────────────────────┼───────────────┼─────────┼──────────┼─────────────┼─────────────────┼───────────────┤
│ <span class="c1">#4                 │ 0.00143      │ conv(FUSE_ADD_RELU[346],multi_ │ 205.52        │ 881.88  │ 9.15     │ 38.34       │ {1,1024,14,14}  │ {1,2048,7,7}  │</span>
│ <span class="m">0</span>.000233           │ <span class="m">12</span>.1%        │ -  dv<span class="o">[</span><span class="m">0</span><span class="o">]</span>:o91<span class="o">)[</span><span class="m">361</span><span class="o">]</span>             │ MFLO          │ GFLOPS  │ MiB      │ GiB/s       │ <span class="o">{</span><span class="m">2048</span>,1024,1,1<span class="o">}</span> │               │
│ <span class="m">2</span>.0%               │              │ ConvolutionForward             │               │         │          │             │                 │               │
│                    │              │ <span class="m">361</span>                            │               │         │          │             │                 │               │
╘════════════════════╧══════════════╧════════════════════════════════╧═══════════════╧═════════╧══════════╧═════════════╧═════════════════╧═══════════════╛
</pre></div>
</div>
</div></blockquote>
<p>这个表格打印了前五个耗时最多的算子。每列的含义如下：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">self</span> <span class="pre">time</span></code> 是算子在计算设备上（例如 GPU ）的运行时间</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cumulative</span></code> 累加前面所有算子的时间</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">operator</span> <span class="pre">info</span></code> 打印算子的基本信息</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">computation</span></code> 是算子需要的浮点数操作数目</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FLOPS</span></code> 是算子每秒执行的浮点操作数目，由 <code class="docutils literal notranslate"><span class="pre">computation</span></code> 除以 <code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">self</span> <span class="pre">time</span></code> 并转换单位得到</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">memory</span></code> 是算子使用的存储（例如 GPU 显存）大小</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bandwidth</span></code> 是算子的带宽，由 <code class="docutils literal notranslate"><span class="pre">memory</span></code> 除以 <code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">self</span> <span class="pre">time</span></code> 并转换单位得到</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">in_shapes</span></code> 是算子输入张量的形状</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_shapes</span></code> 是算子输出张量的形状</p></li>
</ul>
</div>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>

    
  <script src="../../_static/js/index.3c6125c0ae68274ddd1b.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2020-2021, The MegEngine Open Source Team.<br/>
        由 <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.3 创建。<br/>
    </p>
  </div>
</footer>
  </body>
</html>